{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Refer to `Example script.ipynb` for examples of using this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE THE MODEL PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = '/data00/projects/megameta/scripts/jupyter_megameta/l1analysis/darpa1/task-share_model-test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update first level model script and singularity paths if necessary (unlikely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = '/data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/l2analysis_SPM.py'\n",
    "singularity_path = '/data00/tools/singularity_images/neurodocker.sqsh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE SLURM JOBS FOR SECOND-LEVEL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, copy, re, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_exist(path, file):\n",
    "    if os.path.exists(os.path.join(path, file)):\n",
    "        return file\n",
    "    else:\n",
    "        raise Exception(\"File missing: \" + file)\n",
    "\n",
    "def ensure_relative(path):\n",
    "    if path.startswith('/'):\n",
    "        raise Exception(\"Make sure path is relative to the data path: \" + path)\n",
    "    else:\n",
    "        return path\n",
    "        \n",
    "def ensure_list(obj):\n",
    "    if type(obj) is not list:\n",
    "        return [obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def remove_key(keys, obj):\n",
    "    for key in ensure_list(keys):\n",
    "        if obj.get(key):\n",
    "            del obj[key]\n",
    "        \n",
    "def copy_from_template(target, template):\n",
    "    for key, item in template.items():\n",
    "        if key not in target.keys():\n",
    "            target[key] = copy.deepcopy(template[key])\n",
    "        elif type(item) is dict:\n",
    "            copy_from_template(target[key], template[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template template-megameta.json: Mega-meta default pipeline\n",
      "\t4mm FWHM smoothing\n",
      "\tNo global scaling\n",
      "\tFAST correlation\n",
      "\t6 motion parameters and framewise displacement\n",
      "\tTrash FD >= 0.75\n",
      "Using template task-share.json: darpa1 share\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, 'r') as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "descriptions = model.get(\"Description\", [])\n",
    "\n",
    "# Use copy_from_template to combine templates described in model .json into one .json object\n",
    "for template_path in ensure_list(model.get(\"Template\", [])):\n",
    "   \n",
    "    with open(template_path, 'r') as f:\n",
    "        template = json.load(f)\n",
    "    \n",
    "    print(f\"Using template {os.path.basename(template_path)}: \", end=\"\")\n",
    "    print(\"\\n\\t\".join(ensure_list(template.get(\"Description\", [\"No description found\"]))))\n",
    "\n",
    "    descriptions += ensure_list(template.get(\"Description\", []))\n",
    "    \n",
    "    copy_from_template(model, template)\n",
    "    \n",
    "if len(descriptions) > 0:\n",
    "    model['Description'] = descriptions\n",
    "\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take task name to define job name, create output and working directories, and changes permissions\n",
    "task = model['Info']['task']\n",
    "job_name = 'task-{}_model-{}'.format(model['Info']['task'], model['Info']['model'])\n",
    "\n",
    "env = model['Environment']\n",
    "\n",
    "os.makedirs(env['output_path'], exist_ok=True)\n",
    "os.makedirs(env['working_path'], exist_ok=True)\n",
    "\n",
    "# Sets permissions for output and working directories to 777 - all users all permissions \n",
    "try:\n",
    "    os.chmod(env['output_path'], 0o0777)\n",
    "    os.chmod(env['working_path'], 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "l2_path = os.path.join(env['output_path'], job_name)\n",
    "\n",
    "if not os.path.exists(l2_path):\n",
    "    raise Exception(f\"First-level output directory not found: {l2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating contrast files  \n",
      "                     sub\n",
      "con                     \n",
      "clue_Content          41\n",
      "clue_Friend           41\n",
      "clue_Read             41\n",
      "clue_Wall             41\n",
      "post_button_Content   41\n",
      "post_button_Friend    41\n",
      "post_button_Read      41\n",
      "post_button_Wall      41\n",
      "rate_Content          41\n",
      "rate_Friend           41\n",
      "rate_Read             41\n",
      "rate_Wall             41\n",
      "read_Content          41\n",
      "read_Friend           41\n",
      "read_Read             41\n",
      "read_Wall             41\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "con_path = os.path.join(env['output_path'], job_name, 'contrasts.csv')\n",
    "print(\"Aggregating contrast files\", end=\" \")\n",
    "\n",
    "if os.path.exists(con_path) and model.get('SecondLevel',{}).get('force_update', False) == False:\n",
    "    con_df = pd.read_csv(con_path)\n",
    "    \n",
    "else:\n",
    "    con_df = []\n",
    "\n",
    "    for sub_folder in glob.glob(os.path.join(env['output_path'], job_name, 'sub-*')):\n",
    "        print(\"*\",end=\"\")\n",
    "        \n",
    "        if not os.path.isdir(sub_folder):\n",
    "            continue\n",
    "\n",
    "        sub = os.path.basename(sub_folder)\n",
    "\n",
    "        spm_file = os.path.join(sub_folder, 'SPM.mat')\n",
    "        if not os.path.exists(spm_file):\n",
    "            continue\n",
    "\n",
    "        spm_mat = sio.loadmat(spm_file, \n",
    "                   squeeze_me=True, struct_as_record=False)\n",
    "\n",
    "        for n, con in enumerate(spm_mat['SPM'].xCon, start=1):\n",
    "            con_file = os.path.join(sub_folder, f\"con_{n:04d}.nii\")\n",
    "            if os.path.exists(con_file):\n",
    "                con_df.append({'sub': sub,\n",
    "                              'con': con.name,\n",
    "                              'path': con_file.replace(os.path.join(env['output_path'], job_name) + os.path.sep, '')})\n",
    "\n",
    "    con_df = pd.DataFrame(con_df)\n",
    "    con_df.to_csv(con_path, index=False)\n",
    "    \n",
    "print(\" \")    \n",
    "print(con_df.pivot_table(index='con', values='sub', aggfunc='count'))\n",
    "print(\"----\")\n",
    "\n",
    "con_df = con_df.pivot(index=\"sub\", columns='con', values='path').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneSampleTTestDesign_read_Read : job created\n",
      "MultipleRegressionDesign_read_Read : job created\n",
      "PairedTTestDesign_read_Wall_read_Content : job created\n",
      "TwoSampleTTestDesign_read_Wall : job created\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "env['job_path'] = ensure_relative(env['job_path'])\n",
    "\n",
    "job_path = os.path.join(env['data_path'], env['job_path'], job_name, 'jobs') \n",
    "os.makedirs(job_path, exist_ok=True)\n",
    "\n",
    "l2_jobs = ensure_list(model['SecondLevel']['job'])\n",
    "\n",
    "for l2_job in l2_jobs:    \n",
    "    issues = []\n",
    "\n",
    "    l2_name = l2_job.get('name', '{analysis}_{contrast}').format(analysis = l2_job['analysis'],\n",
    "                                                               contrast = \"_\".join(ensure_list(l2_job['contrast'])))\n",
    "    l2_name = re.sub(r'\\W+', '_', l2_name)\n",
    "\n",
    "    l2_job['name'] = l2_name\n",
    "    print(l2_name, \": \", end=\"\")\n",
    "    \n",
    "    output_path = os.path.join(job_name, f'l2-{l2_name}')\n",
    "    os.makedirs(os.path.join(env['output_path'], output_path), exist_ok=True)\n",
    "\n",
    "    l2_analysis = l2_job['analysis']\n",
    "    if l2_analysis not in ['OneSampleTTestDesign', 'PairedTTestDesign', 'TwoSampleTTestDesign', 'MultipleRegressionDesign']:\n",
    "        raise Exception(\"Only these analyses available: OneSampleTTestDesign, PairedTTestDesign, TwoSampleTTestDesign, MultipleRegressionDesign\")\n",
    "           \n",
    "    l2_job['contrast'] = ensure_list(l2_job['contrast'])\n",
    "        \n",
    "    l2_contrasts = ensure_list(l2_job.get('l2_contrasts', []))\n",
    "    conestimate = {}\n",
    "    \n",
    "    if l2_job.get(\"explicit_mask_file\"):\n",
    "        shutil.copyfile(l2_job[\"explicit_mask_file\"], os.path.join(env['output_path'], output_path))\n",
    "        l2_job[\"explicit_mask_file\"] = os.path.join(output_path, l2_job[\"explicit_mask_file\"])\n",
    "        \n",
    "    if l2_analysis in ['OneSampleTTestDesign', 'MultipleRegressionDesign']:\n",
    "        if len(l2_job['contrast']) != 1:\n",
    "            raise Exception(f\"Only one contrast allowed for {l2_analysis}.\")\n",
    "            \n",
    "        selected_con_df = con_df[['sub'] + l2_job['contrast']].dropna()\n",
    "        \n",
    "        if l2_job.get('include_intercept', True):\n",
    "            l2_contrast_names = ['mean']\n",
    "        else:\n",
    "            l2_contrast_names = []\n",
    "\n",
    "        if l2_job.get(\"covariate_file\"):\n",
    "            \n",
    "            if l2_job['covariate_file'].endswith('tsv'):\n",
    "                covariate_df = pd.read_csv(l2_job['covariate_file'], sep='\\t')\n",
    "            else:\n",
    "                covariate_df = pd.read_csv(l2_job['covariate_file'])\n",
    "            \n",
    "            covariate_names = l2_job.get('covariate_names', list(covariate_df.columns))\n",
    "            if 'sub' in covariate_names:\n",
    "                covariate_names.remove('sub')\n",
    "            \n",
    "            selected_con_df = selected_con_df.merge(covariate_df[['sub'] + covariate_names], on='sub', how='left').dropna()\n",
    "\n",
    "            covariate_file = os.path.join(output_path, 'covariates.csv')\n",
    "            l2_job['covariate_file'] = covariate_file\n",
    "            \n",
    "            selected_con_df[['sub'] + covariate_names].to_csv(os.path.join(env['output_path'], covariate_file), index=False)\n",
    "\n",
    "            l2_contrast_names += covariate_names\n",
    "            \n",
    "        l2_job['in_files'] = (job_name + os.path.sep + selected_con_df[l2_job['contrast'][0]]).to_list()\n",
    "        \n",
    "        for cn in l2_contrast_names:\n",
    "            l2_contrasts.append([cn, 'T', [cn], [1]])    \n",
    "\n",
    "    elif l2_analysis == 'PairedTTestDesign':       \n",
    "\n",
    "        if len(l2_job['contrast']) != 2:\n",
    "            raise Exception(f\"Only two contrasts allowed for {l2_analysis}.\")\n",
    "\n",
    "        selected_con_df = con_df[['sub'] + l2_job['contrast']].dropna()\n",
    "\n",
    "        if l2_job.get(\"covariate_file\"):\n",
    "            raise Exception(\"Covariates for PairedTTestDesign not supported yet.\")\n",
    "\n",
    "        cond1_files = (job_name + os.path.sep + selected_con_df[l2_job['contrast'][0]]).to_list()\n",
    "        cond2_files = (job_name + os.path.sep + selected_con_df[l2_job['contrast'][1]]).to_list()\n",
    "        \n",
    "        l2_job['paired_files'] = [[c1,c2] for c1, c2 in zip(cond1_files, cond2_files)]\n",
    "        \n",
    "        #l2_contrasts.append([l2_job['contrast'][0], 'T', ['Condition_{1}'], [1]])\n",
    "        #l2_contrasts.append([l2_job['contrast'][1], 'T', ['Condition_{2}'], [1]])\n",
    "        l2_contrasts.append(['>'.join(l2_job['contrast']), 'T', ['Condition_{1}','Condition_{2}'], [1,-1]])\n",
    "\n",
    "    elif l2_analysis == 'TwoSampleTTestDesign':\n",
    "        \n",
    "        if len(l2_job['contrast']) != 1:\n",
    "            raise Exception(f\"Only one contrast allowed for {l2_analysis}.\")\n",
    "\n",
    "        selected_con_df = con_df[['sub'] + l2_job['contrast']].dropna()\n",
    "        \n",
    "        if l2_job.get(\"groups\"):\n",
    "            \n",
    "            group1_con_df = selected_con_df[selected_con_df['sub'].isin(l2_job['groups'][0])]\n",
    "            group2_con_df = selected_con_df[selected_con_df['sub'].isin(l2_job['groups'][1])]\n",
    "            \n",
    "            l2_job['group1_files'] = (job_name + os.path.sep + group1_con_df[l2_job['contrast'][0]]).to_list()\n",
    "            l2_job['group2_files'] = (job_name + os.path.sep + group2_con_df[l2_job['contrast'][0]]).to_list()\n",
    "            \n",
    "            l2_group_names = ['Group1', 'Group2']\n",
    "            \n",
    "        elif l2_job.get(\"covariate_file\"):\n",
    "\n",
    "            grouping_var = l2_job['grouping_variable']\n",
    "\n",
    "            if l2_job.get('covariate_names'):\n",
    "                raise Exception(\"Covariates for TwoSampleTTestDesign not supported yet.\")\n",
    "\n",
    "            if l2_job['covariate_file'].endswith('tsv'):\n",
    "                covariate_df = pd.read_csv(l2_job['covariate_file'], sep='\\t')[['sub',grouping_var]]\n",
    "            else:\n",
    "                covariate_df = pd.read_csv(l2_job['covariate_file'])[['sub',grouping_var]]\n",
    "            \n",
    "            selected_con_df = selected_con_df.merge(covariate_df, on='sub', how='left').dropna()\n",
    "            \n",
    "            if len(selected_con_df[grouping_var].unique()) != 2:\n",
    "                raise Exception(\"Grouping variable does not have exactly two levels.\")\n",
    "                \n",
    "            group_files = []\n",
    "            l2_group_names = []\n",
    "            \n",
    "            for group, group_df in selected_con_df.groupby(grouping_var):\n",
    "                group_files.append((job_name + os.path.sep + group_df[l2_job['contrast'][0]]).to_list())\n",
    "                l2_group_names.append(f\"{grouping_var} = {group}\")\n",
    "                \n",
    "            l2_job['group1_files'] = group_files[0]\n",
    "            l2_job['group2_files'] = group_files[1]\n",
    "\n",
    "            remove_key(\"covariate_file\", l2_job)\n",
    "        \n",
    "        l2_contrasts.append([l2_group_names[0], 'T', ['Group_{1}'], [1]])\n",
    "        l2_contrasts.append([l2_group_names[1], 'T', ['Group_{2}'], [1]])\n",
    "        l2_contrasts.append(['>'.join(l2_group_names), 'T', ['Group_{1}','Group_{2}'], [1,-1]])\n",
    "    \n",
    "    remove_key([\"contrast\", \"covariate_names\", \"groups\", \"grouping_variable\", \"l2_contrasts\"], l2_job)\n",
    "        \n",
    "    job = { 'Environment': copy.deepcopy(model['Environment']),\n",
    "            'Info': copy.deepcopy(model['Info']),\n",
    "            'SecondLevel': l2_job,\n",
    "            'EstimateContrast': { 'contrasts': l2_contrasts } \n",
    "          }\n",
    "    \n",
    "    job['Environment']['data_path'] = env['output_path']\n",
    "    \n",
    "    remove_key(\"job_path\", job[\"Environment\"])\n",
    "    remove_key([\"sub_container\", \"exclude\", \"run\", \"tr\"], job[\"Info\"])\n",
    "\n",
    "    if len(issues) == 0:\n",
    "        print(\"job created\")\n",
    "    else:\n",
    "        print(\"issues found - \\n\\t\" + \"\\n\\t\".join(issues))\n",
    "\n",
    "    job_output = os.path.join(job_path, f\"l2-{l2_name}.json\")\n",
    "    with open(job_output, 'w') as f:\n",
    "        json.dump(job, f)\n",
    "        \n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_path = os.path.join(env['data_path'], env['job_path'], job_name, 'slurm') \n",
    "os.makedirs(slurm_path, exist_ok=True)\n",
    "try:\n",
    "    os.chmod(slurm_path, 0o0777)\n",
    "    \n",
    "    os.makedirs(os.path.join(slurm_path, 'out'), exist_ok=True)\n",
    "    os.chmod(os.path.join(slurm_path, 'out'), 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "for l2_job in l2_jobs:\n",
    "    \n",
    "    l2_name = l2_job['name']\n",
    "    \n",
    "    slurm_header = []\n",
    "    slurm_header.append(\"#!/bin/bash\")\n",
    "    slurm_header.append(f\"#SBATCH --job-name=l2-{l2_name}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --output=out/l2-{l2_name}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --error=out/l2-{l2_name}.err\")\n",
    "    slurm_header.append(\"#SBATCH --time=5-00:00\")\n",
    "    slurm_header.append(\"\")\n",
    "    slurm_header.append(\"srun \")\n",
    "\n",
    "    data_path = env['output_path']\n",
    "    output_path = env['output_path']\n",
    "    working_path = env['working_path']\n",
    "\n",
    "    json_path = os.path.join(job_path, f\"l2-{l2_name}.json\")\n",
    "    \n",
    "    cmd = []\n",
    "    cmd.append(\"singularity run --cleanenv\")\n",
    "    cmd.append(f\"-B {script_path}:/worker.py\")\n",
    "    cmd.append(f\"-B {data_path}:/data\")\n",
    "    cmd.append(f\"-B {output_path}:/output\")\n",
    "    cmd.append(f\"-B {working_path}:/working\")\n",
    "    cmd.append(f\"-B {json_path}:/job.json\")\n",
    "    cmd.append(f\"{singularity_path} python /worker.py /job.json\")\n",
    "    \n",
    "    slurm_output = os.path.join(output_path, job_name, f\"l2-{l2_name}\")\n",
    "    slurm_working = os.path.join(working_path, job_name, f\"l2-{l2_name}\")\n",
    "    \n",
    "    slurm_footer = []\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_output}\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_working}\")\n",
    "    \n",
    "    slurm_cmd = \"\\n\".join(slurm_header) + \" \\\\\\n  \".join(cmd) + \"\\n\".join(slurm_footer) \n",
    "    \n",
    "    with open(os.path.join(slurm_path, f\"l2-{l2_name}.job\"), 'w') as f:\n",
    "        f.write(slurm_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DRIVE ONE MODEL (SLURM JOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test drive one job (l2-TwoSampleTTestDesign_read_Wall), copy and paste the following lines in terminal:\n",
      "\n",
      "singularity run --cleanenv \\\n",
      "  -B /data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/l2analysis_SPM.py:/worker.py \\\n",
      "  -B /data00/projects/megameta/darpa1/derivatives/nipype:/data \\\n",
      "  -B /data00/projects/megameta/darpa1/derivatives/nipype:/output \\\n",
      "  -B /data00/projects/megameta/darpa1/working/nipype:/working \\\n",
      "  -B /data00/projects/megameta/darpa1/models/task-share_model-test/jobs/l2-TwoSampleTTestDesign_read_Wall.json:/job.json \\\n",
      "  /data00/tools/singularity_images/neurodocker.sqsh python /worker.py /job.json\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(f\"To test drive one job (l2-{l2_name}), copy and paste the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\" \\\\\\n  \".join(cmd))\n",
    "\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN ALL MODELS (SLURM JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\n",
      "\n",
      "cd /data00/projects/megameta/darpa1/models/task-share_model-test/slurm\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-test/slurm -c 8 l2-OneSampleTTestDesign_read_Read.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-test/slurm -c 8 l2-MultipleRegressionDesign_read_Read.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-test/slurm -c 8 l2-PairedTTestDesign_read_Wall_read_Content.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-test/slurm -c 8 l2-TwoSampleTTestDesign_read_Wall.job\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"cd {slurm_path}\")\n",
    "for l2_job in l2_jobs:\n",
    "    \n",
    "    l2_name = l2_job['name']\n",
    "    print(f\"sbatch -D {slurm_path} -c 8 l2-{l2_name}.job\")\n",
    "print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
