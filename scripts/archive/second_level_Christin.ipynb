{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building second level models using _nipype_ and _SPM12_\n",
    "\n",
    "## Base functionality for _megameta_ project\n",
    "\n",
    "-------\n",
    "#### History\n",
    "* 5/4/19 cscholz - add datasink, incorporate mreg design, incorporate sampling of first-level contrast based on percentage of available first-level models per project\n",
    "* 4/15/19 mbod - incorporate function to read the 2nd level JSON model config\n",
    "* 4/9/19 mbod - modify template to work with fmriprep processed data\n",
    "* 3/20/19 mbod - initial setup for testing some simple one sample t-test models\n",
    "-----\n",
    "\n",
    "### Description\n",
    "\n",
    "* Set up a nipype workflow to use SPM12 to make second level models for _megameta_ task data (preprocessed using `batch8` SPM8 scripts) in BIDS derivative format   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200520-08:52:47,204 nipype.utils INFO:\n",
      "\t No new version available.\n"
     ]
    }
   ],
   "source": [
    "import os  # system functions\n",
    "\n",
    "# NIYPE FUNCTIONS\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.interfaces.spm as spm          # spm\n",
    "import nipype.interfaces.matlab as mlab      # how to run matlab\n",
    "import nipype.interfaces.utility as util     # utility\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_SPEC_FILE = 'group_pmod_pop_rank.json'\n",
    "#CONTRAST_NAME='puremessageXpmod_pop_rank'\n",
    "#PATH_TO_SPM_FOLDER = '/data00/tools/spm12mega/'\n",
    "#exclude_subjects=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matlab path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the way matlab should be called\n",
    "mlab.MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "# If SPM is not in your MATLAB path you should add it here\n",
    "mlab.MatlabCommand.set_default_paths(PATH_TO_SPM_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_DIR = '/data00/projects/megameta/group_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_MODEL_FILE = os.path.join('/data00/projects/megameta/scripts/jupyter_megameta/second_level_models',\n",
    "                               'model_specifications',\n",
    "                               MODEL_SPEC_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_MODEL_FILE) as fh:\n",
    "    model_def = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = model_def['ModelName']\n",
    "\n",
    "CONTRASTS = model_def['Contrasts']\n",
    "\n",
    "ROOT_DIR = '/data00/projects/megameta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_contrast_list = CONTRASTS # list of specific contrast files to use in 2nd level model (include .nii?)\n",
    "\n",
    "\n",
    "output_dir = os.path.join(GROUP_DIR,'derivatives', 'nipype','model_2nd-level_{}'.format(MODEL_NAME))        \n",
    "working_dir = os.path.join(GROUP_DIR, 'working', \n",
    "                           'nipype', 'workingdir_model_2nd-level_{}'.format(MODEL_NAME))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of contrast files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_project(project_name, model_def=model_def, exclude_subjects=exclude_subjects ,scan_all_subjs=False, DEBUG=False):\n",
    "    \n",
    "    project_spec = [pspec for pspec in model_def['Projects'] if pspec['Name']==project_name]\n",
    "    \n",
    "    if not project_spec:\n",
    "        print('Cannot find specification for project: ', project_name)\n",
    "        return None\n",
    "    \n",
    "    model_name = project_spec[0]['Model']\n",
    "    cmap = project_spec[0]['ContrastMap']\n",
    "    \n",
    "    \n",
    "    model_dir = os.path.join(ROOT_DIR, project_name, \n",
    "                             \"derivatives\", \"nipype\",\n",
    "                             \"model_{}\".format(model_name)\n",
    "                            )\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        print('Cannot find first level model directory:', model_dir)\n",
    "        return None\n",
    "    \n",
    "    subjs_with_models = [s for s in os.listdir(model_dir) if s.startswith('sub-')]\n",
    "    #exclude_people\n",
    "    subjs_with_models=[s for s in subjs_with_models if s not in exclude_subjects]\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Found {} first level subject models\\n\".format(len(subjs_with_models)))\n",
    "    \n",
    "    \n",
    "    contrast_lists = { cname: [] for cname in cmap}\n",
    "    \n",
    "    \n",
    "    model_contrasts=None\n",
    "    for sidx,subj in enumerate(subjs_with_models):\n",
    "        \n",
    "        if DEBUG:\n",
    "            print('Processing',subj, '-',end='')\n",
    "        \n",
    "        first_level_dir = os.path.join(model_dir, subj, 'medium', 'fwhm_8')\n",
    "\n",
    "        if scan_all_subjs or sidx==0:\n",
    "            spm_mat_file = os.path.join(first_level_dir, 'SPM.mat')\n",
    "\n",
    "            SPM = sio.loadmat(spm_mat_file, squeeze_me=True, struct_as_record=False)['SPM']\n",
    "\n",
    "            model_contrasts = SPM.xCon\n",
    "\n",
    "        if DEBUG:\n",
    "            print(' found {} contrasts'.format(len(model_contrasts)))\n",
    "\n",
    "        con_map = {con.name: 'con_{:0>4}.nii'.format(cidx) for cidx,con in enumerate(model_contrasts,1) }\n",
    "\n",
    "\n",
    "        if DEBUG:\n",
    "            print('\\tContrasts are:', con_map)\n",
    "        \n",
    "        for model_con, proj_con in cmap.items():\n",
    "            \n",
    "            path_to_con = os.path.join(first_level_dir, con_map[proj_con])\n",
    "            \n",
    "            if os.path.exists(path_to_con):\n",
    "                contrast_lists[model_con].append(path_to_con)\n",
    "            \n",
    "    return contrast_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "l2_infosource = pe.Node(util.IdentityInterface(fields=['contrast_id']),\n",
    "                  name=\"infosource\")\n",
    "\n",
    "smoothing_kernels = [ 8 ]\n",
    "resolutions = ['medium']\n",
    "\n",
    "resolution_and_kernel_list = product(resolutions, smoothing_kernels)\n",
    "\n",
    "\n",
    "l2_infosource.iterables = [('contrast_id', l2_contrast_list), \n",
    "                           ('resolution_and_smoothing', resolution_and_kernel_list)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectFiles - to grab the data (alternativ to DataGrabber)\n",
    "\n",
    "subject_pattern='*'\n",
    "OUTPUT_DIR = output_dir\n",
    "l2_output_dir = output_dir\n",
    "\n",
    "l2_templates = {'cons': os.path.join(output_dir, MODEL_NAME, subject_pattern, '{smoothing_ksize}',\n",
    "                         '{contrast_id}.nii')}\n",
    "\n",
    "l2_selectfiles = pe.Node(nio.SelectFiles(l2_templates,\n",
    "                               base_directory=OUTPUT_DIR,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contrast_list(model_path, cname,exclude_subjects, sample_perc=80):\n",
    "    #EDITED BY CHRISTIN to get randomly sample a given percentage of subjects for second-level model\n",
    "\n",
    "    import json\n",
    "    import random\n",
    "    import os\n",
    "    import scipy.io as sio\n",
    "    \n",
    "    ROOT_DIR = '/data00/projects/megameta'\n",
    "    \n",
    "    def process_project(project_name, model_def, scan_all_subjs=False, DEBUG=False):\n",
    "\n",
    "        project_spec = [pspec for pspec in model_def['Projects'] if pspec['Name']==project_name]\n",
    "\n",
    "        if not project_spec:\n",
    "            print('Cannot find specification for project: ', project_name)\n",
    "            return None\n",
    "\n",
    "        model_name = project_spec[0]['Model']\n",
    "        cmap = project_spec[0]['ContrastMap']\n",
    "\n",
    "\n",
    "        model_dir = os.path.join(ROOT_DIR, project_name, \n",
    "                                 \"derivatives\", \"nipype\",\n",
    "                                 \"model_{}\".format(model_name)\n",
    "                                )\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            print('Cannot find first level model directory:', model_dir)\n",
    "            return None\n",
    "\n",
    "        subjs_with_models = [s for s in os.listdir(model_dir) if s.startswith('sub-')]\n",
    "        #Exclude people\n",
    "        subjs_with_models=[s for s in subjs_with_models if s not in exclude_subjects]\n",
    "        \n",
    "        #Get a random sample of participants (based on a percentage)\n",
    "        sample_size=(sample_perc/100)*len(subjs_with_models)\n",
    "        subj_list=random.sample(subjs_with_models,int(sample_size))\n",
    "        \n",
    "        print('Project: {}, Sampling {} of {} participants with a model'.format(project_name, int(sample_size), len(subjs_with_models)))\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"Found {} first level subject models\\n\".format(len(subjs_with_models)))\n",
    "\n",
    "\n",
    "        contrast_lists = { cname: [] for cname in cmap}\n",
    "\n",
    "\n",
    "        model_contrasts=None\n",
    "        for sidx,subj in enumerate(subj_list):\n",
    "\n",
    "            if DEBUG:\n",
    "                print('Processing',subj, '-',end='')\n",
    "\n",
    "            first_level_dir = os.path.join(model_dir, subj, 'medium', 'fwhm_8')\n",
    "\n",
    "            if scan_all_subjs or sidx==0:\n",
    "                spm_mat_file = os.path.join(first_level_dir, 'SPM.mat')\n",
    "\n",
    "                SPM = sio.loadmat(spm_mat_file, squeeze_me=True, struct_as_record=False)['SPM']\n",
    "\n",
    "                model_contrasts = SPM.xCon\n",
    "\n",
    "            if DEBUG:\n",
    "                print(' found {} contrasts'.format(len(model_contrasts)))\n",
    "\n",
    "            con_map = {con.name: 'con_{:0>4}.nii'.format(cidx) for cidx,con in enumerate(model_contrasts,1) }\n",
    "\n",
    "\n",
    "            if DEBUG:\n",
    "                print('\\tContrasts are:', con_map)\n",
    "\n",
    "            for model_con, proj_con in cmap.items():\n",
    "\n",
    "                path_to_con = os.path.join(first_level_dir, con_map[proj_con])\n",
    "\n",
    "                if os.path.exists(path_to_con):\n",
    "                    contrast_lists[model_con].append(path_to_con)\n",
    "\n",
    "        return contrast_lists, subjs_with_models\n",
    "\n",
    "    with open(model_path) as fh:\n",
    "        model_def = json.load(fh)\n",
    "        \n",
    "    conlist=[]\n",
    "    for p in model_def['Projects']:\n",
    "        print(p)\n",
    "        conlist.extend(process_project(p['Name'], model_def)[cname])\n",
    "        \n",
    "    return conlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_getcontrasts = pe.Node(util.Function(input_names=['model_path','cname','exclude_subjects'],\n",
    "                                     output_names=['contrasts', 'covariates'],\n",
    "                                      function=make_contrast_list),\n",
    "                    name='makecontrasts')\n",
    "MDIR = os.path.abspath('../model_specifications')\n",
    "l2_getcontrasts.inputs.model_path=os.path.join(MDIR, MODEL_SPEC_FILE)\n",
    "l2_getcontrasts.inputs.cname=CONTRAST_NAME\n",
    "l2_getcontrasts.inputs.exclude_subjects=exclude_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITED BY CHRISTIN (ADDING DATASINK)\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = pe.Node(nio.DataSink(base_directory=OUTPUT_DIR,\n",
    "                         container=l2_output_dir),\n",
    "                name=\"datasink\")\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_contrast_id_', '')]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "osttdesign = pe.Node(spm.model.OneSampleTTestDesign(),\n",
    "                         name=\"osttdesign\")\n",
    "\n",
    "osttdesign.inputs.explicit_mask_file='/data00/tools/spm8/apriori/brainmask_th25.nii'\n",
    "osttdesign.inputs.threshold_mask_none=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_SPEC_FILE = 'group_mreg_behav_nonavers.json'\n",
    "#CONTRAST_NAME='puremessage'\n",
    "#PATH_TO_SPM_FOLDER = '/data00/tools/spm12mega/'\n",
    "#JSON_MODEL_FILE = os.path.join('/data00/projects/megameta/scripts/jupyter_megameta/second_level_models',\n",
    "#                               'model_specifications',\n",
    "#                               MODEL_SPEC_FILE)\n",
    "#exclude_subjects=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'PA1', 'Model': 'MESSAGE_pureexposure', 'ContrastMap': {'puremessage': 'pure_message', 'impuremessage': 'impure_message'}}\n",
      "Project: PA1, Sampling 39 of 49 participants with a model\n",
      "{'Name': 'PA2', 'Model': 'MESSAGE_pureexposure', 'ContrastMap': {'puremessage': 'pure_message', 'impuremessage': 'impure_message'}}\n",
      "Project: PA2, Sampling 151 of 189 participants with a model\n",
      "['PA1', 'PA2']\n"
     ]
    }
   ],
   "source": [
    "#EDITED BY CHRISTIN TO IMPPLEMENT MREG\n",
    "\n",
    "# Multiple Regression Design - creates mreg Design\n",
    "mregdesign = pe.Node(spm.model.MultipleRegressionDesign(),\n",
    "                         name=\"mregdesign\")\n",
    "# Add covariates\n",
    "## Make a list of covariates based on the contrast list\n",
    "covs=[]\n",
    "contrast_list, subj_list=make_contrast_list(JSON_MODEL_FILE,CONTRAST_NAME,exclude_subjects)[0]\n",
    "pjs=[c.split('/')[4] for c in contrast_list]\n",
    "pjs=[s for s in set(pjs)]\n",
    "print(pjs)\n",
    "print(subj_list)\n",
    "\n",
    "## Make dummy variables based on list of projects and add them to the covariate list of dictionaries\n",
    "#for pj in set(pjs):\n",
    "#    cur_cov_vector=[]\n",
    "#    for idx, _ in enumerate(pjs):\n",
    "##            if pjs[idx]==pj:\n",
    " #               cur_cov_vector.append(1)\n",
    " #           else:\n",
    " #               cur_cov_vector.append(0)\n",
    " #   #make dictionary for current covariate\n",
    " #   cur_dict={'name': pj, 'vector': cur_cov_vector}\n",
    " #   #add dictionary to list of covs\n",
    " #   covs.append(cur_dict)\n",
    "\n",
    "##NOTE: THE CODE ABOVE CREATES ONE DUMMY PER PROJECT. NEED TO TAKE ONE OUT AND DECIDE WHICH PROJECT TO USE AS COMPARISON/REFERENCE. \n",
    "#BELOW ARE TWO VERSIONS OF DOING THAT. VERSIN 1 RANDOMLY CHOOSES (# OF PROJECTS)-1 COVARIATES TO INCLUDE - BUT WE PROBABLY WANT TO BE MORE STRATEGIC\n",
    "#VERSION 1\n",
    "#covs=random.sample(covs,(len(pjs)-1))\n",
    "# VERSION 2 REMOVES DARPA1 TO MAKE IT THE REFERENCE PROJECT -- BUT I DON'T HAVE A CLEAR RATIONALE FOR WHY THAT OVER OTHERS RIGHT NOW...\n",
    "#covs=[i for i in covs if i['name']!='darpa1']\n",
    "\n",
    "# Intended covs format:\n",
    "# covs = [\n",
    "#    {'name':'alcohol','vector': []},\n",
    "#    {'name':'darpa1','vector': []},\n",
    "#    {'name':'darpa2','vector': []},\n",
    "#    {'name':'cityyear','vector': []},\n",
    "#    {'name':'project1','vector': []}\n",
    "#]\n",
    "\n",
    "# Add covariate of behaivor change and baseline\n",
    "#subj_list=[]\n",
    "#for pj in pjs:\n",
    "#    project_spec = [pspec for pspec in model_def['Projects'] if pspec['Name']==pj]#\n",
    "\n",
    "    \n",
    "\n",
    "#    model_name = project_spec[0]['Model']\n",
    "#    model_dir = os.path.join(ROOT_DIR, pj, \n",
    "#                            \"derivatives\", \"nipype\",\n",
    "#                            \"model_{}\".format(model_name)\n",
    "#                        )\n",
    "#    subjs_with_models = [s for s in os.listdir(model_dir) if s.startswith('sub-')]\n",
    "#    #Exclude people\n",
    "#    subjs_with_models=[s for s in subjs_with_models if s not in exclude_subjects]\n",
    "#    subj_list=subj_list+subjs_with_models\n",
    "    \n",
    "#subj_list=[s.replace('sub-','') for s in subj_list]\n",
    "\n",
    "##make a new behavior vector for the people who are in subj_list\n",
    "#regressors=pd.read_csv('/data00/projects/megameta/scripts/jupyter_megameta/second_level_models/indbehav_data/behaviorchange_050919nc.csv')\n",
    "#behav_mreg=[]\n",
    "#for row_num, val in enumerate(regressors['change']):\n",
    "#    if regressors['pID'][row_num] in subj_list:\n",
    "#        behav_mreg.append(regressors['change'][row_num])\n",
    "\n",
    "#behav_mreg_dict={'name': 'behav_mreg', 'vector':behav_mreg}\n",
    "\n",
    "#behav_baseline=[]\n",
    "#for row_num, val in enumerate(regressors['baseline']):\n",
    "#    if regressors['pID'][row_num] in subj_list:\n",
    "#        behav_baseline.append(regressors['baseline'][row_num])\n",
    "\n",
    "#behav_baseline_dict={'name': 'behav_baseline', 'vector':behav_baseline}\n",
    "\n",
    "#covs=[behav_mreg_dict,behav_baseline_dict]\n",
    "\n",
    "#mregdesign.inputs.covariates=covs\n",
    "\n",
    "#mregdesign.inputs.explicit_mask_file='/data00/tools/spm8/apriori/brainmask_th25.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstimateModel - estimate the parameters of the model\n",
    "level2estimate = pe.Node(spm.model.EstimateModel(estimation_method={'Classical': 1}),\n",
    "                      name=\"level2estimate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstimateContrast - estimates simple group contrast\n",
    "level2conestimate = pe.Node(spm.model.EstimateContrast(group_contrast=True),\n",
    "                         name=\"level2conestimate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cont1 = ['QuitIntent', 'T', ['QuitIntent', 'FTND', 'mean_WC', 'mean'], [1, 0, 0, 0]]\n",
    "cont2 = ['FTND', 'T', ['QuitIntent', 'FTND', 'mean_WC', 'mean'], [0, 1, 0, 0]]\n",
    "cont3 = ['mean_WC', 'T', ['QuitIntent', 'FTND', 'mean_WC', 'mean'], [0, 0, 1, 0]]\n",
    "cont4 = ['mean', 'T', ['QuitIntent', 'FTND', 'mean_WC', 'mean'], [0, 0, 0, 1]]\n",
    "'''\n",
    "\n",
    "cont = ['behav_mreg', 'T', ['behav_mreg','behav_baseline'], [1,0]]\n",
    "\n",
    "level2conestimate.inputs.contrasts = [cont]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup second level workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l2_working_dir = os.path.join(PROJECT_DIR, 'nipype', 'workingdir_banner_2nd_level')\n",
    "l2_working_dir = working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITED BY CHRISTIN (adding datasink to the workflow)\n",
    "l2analysis = pe.Workflow(name='l2analysis')\n",
    "\n",
    "l2analysis.base_dir = l2_working_dir\n",
    "\n",
    "# Connect up the 2nd-level analysis components\n",
    "l2analysis.connect(\n",
    "                    [\n",
    "                        \n",
    "                    #(l2_infosource, l2_getcontrasts, [('contrast_id', 'contrast_id'),\n",
    "                     #                                ('model_path')]),\n",
    "                     \n",
    "                     (l2_getcontrasts,  mregdesign, [('contrasts', 'in_files')]),\n",
    "                     \n",
    "                    (mregdesign, level2estimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file')] ),\n",
    "                    (level2estimate, level2conestimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file'),\n",
    "                                                         ('beta_images',\n",
    "                                                          'beta_images'),\n",
    "                                                         ('residual_image',\n",
    "                                                          'residual_image')]),\n",
    "                    (level2conestimate, datasink, [('spm_mat_file',\n",
    "                                                    'contrasts.@spm_mat'),\n",
    "                                                   ('spmT_images',\n",
    "                                                    'contrasts.@T'),\n",
    "                                                   ('con_images',\n",
    "                                                    'contrasts.@con')])\n",
    "                    ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
