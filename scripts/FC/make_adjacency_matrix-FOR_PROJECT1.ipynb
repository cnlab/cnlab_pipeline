{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mega-Meta Functional Connectivity Pipeline\n",
    "\n",
    "_________\n",
    "#### Description\n",
    "extracts signal from Power ROI spheres (264) for a given task condition, as defined by a model spec file. Create condition specific adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,glob,sys,pickle,json\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# NIYPE FUNCTIONS\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "from nipype.interfaces.utility import IdentityInterface, Function    # utility\n",
    "from nipype.pipeline.engine import Node\n",
    "from nipype.pipeline.engine.workflows import Workflow\n",
    "\n",
    "\n",
    "from nilearn import image, plotting, input_data\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nipype Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Infosource for iterating over subjects\n",
    "2. create subject information structure\n",
    "3. process confounds\n",
    "4. subset TR\n",
    "5. process signal\n",
    "6. correlation pairwise\n",
    "7. save function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infosource for iterating subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SUBJECT_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7e3e3fcd40e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                  name = 'infosource')\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minfoSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subject_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSUBJECT_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SUBJECT_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "# set up infosource\n",
    "infoSource = Node(IdentityInterface(fields = ['subject_id']),\n",
    "                 name = 'infosource')\n",
    "\n",
    "infoSource.iterables = [('subject_id',SUBJECT_LIST)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subject information\n",
    "This function finds those runs for a given subject that are complete (e.g. have motion, events and functional data). The function then creates the `subject_str` which is a modified `model_str` with subject specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_def' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dc23119212fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                    name = \"get_subject_info\")\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mget_sub_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_def' is not defined"
     ]
    }
   ],
   "source": [
    "def get_subject_info(subject_id,model_str):\n",
    "    \"\"\"\n",
    "    checks what runs a given subject has all information for\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # THIS IS ONE of TWO DIFFERENCES FOR THIS PIPELINE. ONLY TAKES BANNER TASK\n",
    "    model_str['TaskName'] = model_str['TaskName'][0]\n",
    "    model_str['Runs'] = model_str['Runs']['banner']\n",
    "    \n",
    "    subPath = model_str['sub_path'].format(PROJECT=model_str['ProjectID'],PID=subject_id)\n",
    "    \n",
    "\n",
    "    Runs = []\n",
    "    for r in model_str['Runs']:  # THIS IS THE OTHER DIFFERENCE, ONLY TAKES BANNER TASK RUNS\n",
    "        func = model_str['task_func_template'].format(PID=subject_id,\n",
    "                                                      TASK=model_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        motion = model_str['motion_template'].format(PID=subject_id,\n",
    "                                                      TASK=model_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        events = model_str['event_template'].format(PID=subject_id,\n",
    "                                                      TASK=model_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "\n",
    "        # check if files exist\n",
    "        if (os.path.isfile(os.path.join(subPath,func)) and \n",
    "            os.path.isfile(os.path.join(subPath,motion)) and\n",
    "            os.path.isfile(os.path.join(subPath,events))):\n",
    "            Runs.append(r)\n",
    "    \n",
    "    # return a subject modified model_structure\n",
    "    subj_str = model_str\n",
    "    subj_str['subject_id'] = subject_id\n",
    "    subj_str['Runs'] = Runs\n",
    "    \n",
    "    return subj_str\n",
    "            \n",
    "    \n",
    "get_sub_info = Node(Function(input_names=['subject_id','model_str'],\n",
    "                             output_names=['subj_str'],\n",
    "                            function = get_subject_info),\n",
    "                   name = \"get_subject_info\")\n",
    "\n",
    "get_sub_info.inputs.model_str = model_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Confounds\n",
    "This function extracts matter and motion confounds. Matter confounds include Global average signal (from grey matter mask), white matter, and CSF average signal. There are 24 motion parameters, as per Power (2012). These include all 6 motion regressors, their derivatives, the quadratic of the motion params, and the squared derivatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confounds(subject_str):\n",
    "    \"\"\"\n",
    "    extract confounds for all available runs\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import glob\n",
    "    import os\n",
    "    from nilearn import image, input_data\n",
    "    \n",
    "    subPath = subject_str['sub_path'].format(PROJECT=subject_str['ProjectID'],PID=subject_str['subject_id'])\n",
    "    struc_files = glob.glob(subject_str['anat_template'].format(PID=subject_str['subject_id'][4:]))\n",
    "    \n",
    "    # make matter masks\n",
    "    maskers = [input_data.NiftiLabelsMasker(labels_img=struc,standardize=True,memory='nilearn_cache') for struc in struc_files]\n",
    "\n",
    "    confound = {}\n",
    "    for r in subject_str['Runs']:\n",
    "        \n",
    "        func = subject_str['task_func_template'].format(PID=subject_str['subject_id'],\n",
    "                                                      TASK=subject_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        func_file = os.path.join(subPath,func)\n",
    "        \n",
    "        # high variance confounds\n",
    "        hv_confounds = image.high_variance_confounds(func_file)\n",
    "        \n",
    "        # get This runs matter confounds (grand mean, white matter, CSF)\n",
    "        matter_confounds = None\n",
    "        for mask in maskers:\n",
    "            mt = mask.fit_transform(func_file)\n",
    "            mean_matter = np.nanmean(mt,axis=1) # get average signal\n",
    "            \n",
    "            if matter_confounds is None:\n",
    "                matter_confounds = mean_matter\n",
    "            else:\n",
    "                matter_confounds = np.column_stack([matter_confounds,mean_matter])\n",
    "            \n",
    "        # Motion includes xyz,roll,pitch,yaw\n",
    "        # their derivatives, the quadratic term, and qaudratic derivatives\n",
    "        motion = subject_str['motion_template'].format(PID=subject_str['subject_id'],\n",
    "                                                      TASK=subject_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        \n",
    "        motion = np.genfromtxt(os.path.join(subPath,motion),delimiter='\\t',skip_header=True)\n",
    "        motion = motion[:,:6] # dont take framewise displacement\n",
    "        \n",
    "        # derivative of motion\n",
    "        motion_deriv = np.concatenate([np.zeros([1,np.shape(motion)[1]]),np.diff(motion,axis=0)],axis=0)\n",
    "        matter_deriv = np.concatenate([np.zeros([1,np.shape(matter_confounds)[1]]),np.diff(matter_confounds,axis=0)],axis=0)\n",
    "        \n",
    "        conf = np.concatenate([motion,motion**2,motion_deriv,motion_deriv**2,\n",
    "                               matter_confounds,matter_confounds**2,matter_deriv,matter_deriv**2,\n",
    "                               hv_confounds],axis=1)\n",
    "        confound[r] = conf\n",
    "    return confound\n",
    "    \n",
    "confounds = Node(Function(input_names=['subject_str'],\n",
    "                         output_names = ['confound'],\n",
    "                         function = extract_confounds),\n",
    "                name = 'get_confounds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition TR\n",
    "This function finds those TR for a run that match the condition labels of a given model specification. The `condition` input argument must be set for a given pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_TR(subject_str):\n",
    "    \"\"\"\n",
    "    Gets the TR list for condition of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    subPath = subject_str['sub_path'].format(PROJECT=subject_str['ProjectID'],PID=subject_str['subject_id'])\n",
    "    \n",
    "    conditions = subject_str['Conditions'][subject_str['condition']]\n",
    "    \n",
    "    TRs = {}\n",
    "    for r in subject_str['Runs']:\n",
    "        ev = subject_str['event_template'].format(PID=subject_str['subject_id'],\n",
    "                                                      TASK=subject_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        \n",
    "        events_df = pd.read_csv(os.path.join(subPath,ev),delimiter='\\t')\n",
    "        rel_events = events_df.loc[events_df.trial_type.isin(conditions)].reset_index()\n",
    "        \n",
    "        rel_events['TR'] = (rel_events['onset']/subject_str['TR']).astype('int')\n",
    "        rel_events['durTR'] = (rel_events['duration']/subject_str['TR']).astype('int')\n",
    "\n",
    "        condition_TR = []\n",
    "        \n",
    "        for i,tr in enumerate(rel_events.TR):\n",
    "            dur = rel_events.loc[0,'durTR']\n",
    "            condition_TR.extend(list(range(tr,tr+dur)))\n",
    "        TRs[r] = condition_TR\n",
    "        \n",
    "    return TRs\n",
    "\n",
    "events = Node(Function(input_names=['subject_str'],\n",
    "                         output_names = ['TRs'],\n",
    "                         function = get_condition_TR),\n",
    "                name = 'get_TRs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Signal\n",
    "This is where things all come together. Data is masked and confounds are regressed from masked signal. Only those TR for the condition are then subset from the TR. Currently Power atlas is used as a masker (264 nodes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal(subject_str,confound,TRs,mask):\n",
    "    \"\"\"\n",
    "    gets task data, regresses confounds and subsets relevant TR\n",
    "    \"\"\"\n",
    "    \n",
    "    from nilearn import image, input_data\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    subPath = subject_str['sub_path'].format(PROJECT=subject_str['ProjectID'],PID=subject_str['subject_id'])\n",
    "    \n",
    "    signal = None\n",
    "    for r in subject_str['Runs']:\n",
    "        runTR = TRs[r]\n",
    "        con = confound[r]\n",
    "        func = subject_str['task_func_template'].format(PID=subject_str['subject_id'],\n",
    "                                                      TASK=subject_str['TaskName'],\n",
    "                                                      RUN=r)\n",
    "        func_file = os.path.join(subPath,func)\n",
    "        \n",
    "        masked_fun = mask.fit_transform(func_file,con)\n",
    "        \n",
    "        condition_TR = [_ for _ in runTR if _ < masked_fun.shape[0]]\n",
    "        \n",
    "         # if condition is rest, take all TR that are unmodeled\n",
    "        if subject_str['condition'] == 'rest':\n",
    "            masked_condition = masked_fun[[i for i in range(masked_fun.shape[0]) if i not in condition_TR],:]\n",
    "        else:\n",
    "            masked_condition = masked_fun[condition_TR,:]\n",
    "    \n",
    "        if signal is None:\n",
    "            signal = masked_condition\n",
    "        else:\n",
    "            signal = np.concatenate([signal,masked_condition],axis=0)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "\n",
    "signal = Node(Function(input_names=['subject_str','confound','TRs','mask'],\n",
    "                         output_names = ['signal'],\n",
    "                         function = get_signal),\n",
    "                name = 'get_signal')\n",
    "\n",
    "signal.inputs.mask = NODE_MASKER\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrix\n",
    "\n",
    "The final step of the pipeline. Data is pairwise correlated using pearson R and output is a 264X264 adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adj_matrix(signal):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    signal[np.isnan(signal)] = 0\n",
    "    \n",
    "    features = signal.shape[1]\n",
    "    \n",
    "    r_adj = np.zeros([features,features])\n",
    "    p_adj = np.zeros([features,features])\n",
    "    for i in range(features):\n",
    "        for i2 in range(features):\n",
    "            r_adj[i,i2],p_adj[i,i2] = stats.pearsonr(signal[:,i],signal[:,i2])\n",
    "\n",
    "    return r_adj,p_adj\n",
    "\n",
    "adj_matrix = Node(Function(input_names=['signal'],\n",
    "                          output_names = ['r_adj','p_adj'],\n",
    "                          function = make_adj_matrix),\n",
    "                 name = 'adjacency_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data output\n",
    "Output is a json file containing  \n",
    "* the subject ID\n",
    "* Project\n",
    "* Task name\n",
    "* Condition\n",
    "* Pearson r value adj matrix\n",
    "* p.value adj matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_out(subject_str,r_adj,p_adj):\n",
    "    import pickle,os\n",
    "    Output = {\"SubjectID\":subject_str['subject_id'],\n",
    "              \"Project\":subject_str['ProjectID'],\n",
    "              \"Task\":subject_str['TaskName'],\n",
    "              \"Condition\":subject_str['condition'],\n",
    "              'r_adj':r_adj,\n",
    "              'p_adj':p_adj}\n",
    "    \n",
    "    subFile = '{PID}_task-{TASK}_condition-{CONDITION}_parcellation-POWER2011_desc-FCcorrelation_adj.pkl'.format(PID = subject_str['subject_id'],\n",
    "                                                                                          TASK = subject_str['TaskName'],\n",
    "                                                                                                                CONDITION=subject_str['condition'])\n",
    "    outFile = os.path.join(subject_str['output_dir'],subFile)\n",
    "    \n",
    "    with open(outFile,'wb') as outp:\n",
    "        pickle.dump(Output,outp)\n",
    "\n",
    "data_save = Node(Function(input_names=['subject_str','r_adj','p_adj'],\n",
    "                        function = data_out),\n",
    "               name = 'data_out')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "## WIRE UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infoSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0d638975f4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'workflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m wfl.connect([(infoSource,get_sub_info,[(\"subject_id\",\"subject_id\")]),\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mget_sub_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subj_str\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"subject_str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mget_sub_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subj_str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'subject_str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mget_sub_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subj_str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'subject_str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infoSource' is not defined"
     ]
    }
   ],
   "source": [
    "wfl = Workflow(name='workflow')\n",
    "wfl.base_dir = working_dir\n",
    "\n",
    "wfl.connect([(infoSource,get_sub_info,[(\"subject_id\",\"subject_id\")]),\n",
    "            (get_sub_info, confounds,[(\"subj_str\",\"subject_str\")]),\n",
    "            (get_sub_info, events,[('subj_str','subject_str')]),\n",
    "            (get_sub_info,signal,[('subj_str','subject_str')]),\n",
    "            (confounds,signal,[('confound','confound')]),\n",
    "            (events,signal,[('TRs','TRs')]),\n",
    "            (signal, adj_matrix,[('signal','signal')]),\n",
    "            (get_sub_info,data_save,[('subj_str','subject_str')]),\n",
    "            (adj_matrix, data_save,[('r_adj','r_adj'),('p_adj','p_adj')]),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
