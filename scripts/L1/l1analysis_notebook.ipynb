{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Refer to `Example script.ipynb` for examples of using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update first level model script and singularity paths if necessary (unlikely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fmriNASTest/data00/tools/cnlab_pipeline/L1/l1analysis_SPM.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_path = '/data00/tools/cnlab_pipeline/L1' #os.getcwd() (Update with directory L1 is in)\n",
    "script_path = os.path.join(L1_path, 'l1analysis_SPM.py')\n",
    "singularity_path = '/data00/tools/singularity_images/neurodocker.sqsh' #Update with neurodocker path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE SLURM JOBS FOR INDIVIDUAL PARTICIPANT FIRST-LEVEL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_exist(path, file):\n",
    "    if os.path.exists(os.path.join(path, file)):\n",
    "        return file\n",
    "    else:\n",
    "        raise Exception(\"File missing: \" + file)\n",
    "\n",
    "def ensure_relative(path):\n",
    "    if path.startswith('/'):\n",
    "        raise Exception(\"Make sure path is relative to the data path: \" + path)\n",
    "    else:\n",
    "        return path\n",
    "        \n",
    "def ensure_list(obj):\n",
    "    if type(obj) is not list:\n",
    "        return [obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def remove_key(key, obj):\n",
    "    if obj.get(key):\n",
    "        del obj[key]\n",
    "        \n",
    "def copy_from_template(target, template):\n",
    "    for key, item in template.items():\n",
    "        if key not in target.keys():\n",
    "            target[key] = copy.deepcopy(template[key])\n",
    "        elif type(item) is dict:\n",
    "            copy_from_template(target[key], template[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template template-bbprime_beta.json: BBPRIME default pipeline\n",
      "\t6mm FWHM smoothing\n",
      "\tNo global scaling\n",
      "\tFAST correlation\n",
      "\tx_trans, y_trans, z_trans, x_rot, y_rot, z_rot, csf, trash regressor (FD > 0.75 | GS > 3 SD)\n",
      "Using template task-share_beta.json: BBPRIME share task\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, 'r') as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "descriptions = model.get(\"Description\", [])\n",
    "\n",
    "# Use copy_from_template to combine templates described in model .json into one .json object\n",
    "for template_path in ensure_list(model.get(\"Template\", [])):\n",
    "   \n",
    "    with open(template_path, 'r') as f:\n",
    "        template = json.load(f)\n",
    "    \n",
    "    print(f\"Using template {os.path.basename(template_path)}: \", end=\"\")\n",
    "    print(\"\\n\\t\".join(ensure_list(template.get(\"Description\", [\"No description found\"]))))\n",
    "\n",
    "    descriptions += ensure_list(template.get(\"Description\", []))\n",
    "    \n",
    "    copy_from_template(model, template)\n",
    "    \n",
    "if len(descriptions) > 0:\n",
    "    model['Description'] = descriptions\n",
    "\n",
    "remove_key('SecondLevel', model)\n",
    "\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 51 participants:\n",
      "['BPA01', 'BPA02', 'BPA03', 'BPA04', 'BPA05', 'BPA06', 'BPA08', 'BPA09', 'BPA10', 'BPA11', 'BPA12', 'BPA14', 'BPA15', 'BPA16', 'BPA18', 'BPA19', 'BPA21', 'BPA22', 'BPA23', 'BPA26', 'BPA27', 'BPA28', 'BPA29', 'BPA30', 'BPA31', 'BPA32', 'BPA33', 'BPA34', 'BPA35', 'BPA36', 'BPA37', 'BPA38', 'BPA39', 'BPA41', 'BPA42', 'BPA43', 'BPA44', 'BPA45', 'BPA46', 'BPA47', 'BPA48', 'BPA81', 'BPA82', 'BPA84', 'BPA85', 'BPA86', 'BPA87', 'BPA88', 'BPA89', 'BPA90', 'BPA91']\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Take task name to define job name, create output and working directories, and changes permissions\n",
    "task = model['Info']['task']\n",
    "job_name = 'task-{}_model-{}'.format(model['Info']['task'], model['Info']['model'])\n",
    "\n",
    "env = model['Environment']\n",
    "\n",
    "os.makedirs(env['output_path'], exist_ok=True)\n",
    "os.makedirs(env['working_path'], exist_ok=True)\n",
    "\n",
    "# Sets permissions for output and working directories to 777 - all users all permissions \n",
    "try:\n",
    "    os.chmod(env['output_path'], 0o0777)\n",
    "    os.chmod(env['working_path'], 0o0777)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Gather subs directly from template or from sub container (pulling folders from sub container that start with 'sub-')\n",
    "subs = []\n",
    "if model['Info'].get('sub'):\n",
    "    subs = ensure_list(model['Info']['sub'])\n",
    "elif model['Info'].get('sub_container'):\n",
    "    sub_container = ensure_relative(model['Info']['sub_container'])\n",
    "    subs = [x.split(os.path.sep)[-2].replace('sub-','') for x in glob.glob(os.path.join(env['data_path'], sub_container, 'sub-*'+os.path.sep))]\n",
    "\n",
    "# remove excluded subs from sub list    \n",
    "exclude_subs = model['Info'].get('exclude',{}).get('sub',[])\n",
    "for es in exclude_subs:\n",
    "    es = es.replace('sub-','')\n",
    "    if es in subs:\n",
    "        subs.remove(es)\n",
    "\n",
    "if len(subs) == 0:\n",
    "    raise Exception(\"No subjects found.\")\n",
    "else:\n",
    "    print(f\"Processing {len(subs)} participants:\")\n",
    "    print(subs)\n",
    "\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_event(env, job_name, spm_model, event_path, tr_max):\n",
    "    \n",
    "    output_path = os.path.join(ensure_relative(env['job_path']), job_name, 'events') \n",
    "    os.makedirs(os.path.join(env['data_path'], output_path), exist_ok=True)\n",
    "    \n",
    "    event_df = (pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "                .query('onset >= 0')\n",
    "                .query(f'onset <= {tr_max}')\n",
    "                .reset_index(drop=True))\n",
    "    \n",
    "    for operation, params in spm_model.get(\"event_options\", {}).items():\n",
    "        \n",
    "        if operation == 'map_event':\n",
    "            \n",
    "            condition_map = {}\n",
    "            for trial_type in event_df['trial_type'].unique():\n",
    "                condition_map[trial_type] = trial_type\n",
    "                \n",
    "            for new_trial_type, trial_types in params.items():\n",
    "                for trial_type in ensure_list(trial_types):\n",
    "                    condition_map[trial_type] = new_trial_type\n",
    "            \n",
    "            event_df['trial_type'] = event_df['trial_type'].map(condition_map)\n",
    "            \n",
    "        elif operation == 'melt_event':\n",
    "            \n",
    "            for trial_type, trial_data in params.items():\n",
    "                for idx, row in event_df.query(f'trial_type == \"{trial_type}\"').iterrows():\n",
    "                    event_df.loc[idx, 'trial_type'] = trial_type + \"_\" + str(row[trial_data])\n",
    "\n",
    "        elif operation == 'include_event':\n",
    "            \n",
    "            include_events = ensure_list(params)\n",
    "            event_df = event_df[event_df['trial_type'].isin(include_events)].reset_index(drop=True)\n",
    "                                \n",
    "        elif operation == 'exclude_event':\n",
    "            \n",
    "            exclude_events = ensure_list(params)\n",
    "            event_df = event_df[~event_df['trial_type'].isin(exclude_events)].reset_index(drop=True)\n",
    "\n",
    "    all_pmods = []\n",
    "    for trial_type, pmods in spm_model.get('pmod',{}).items():\n",
    "        \n",
    "        trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "        for pmod in ensure_list(pmods):\n",
    "            all_pmods.append(pmod)\n",
    "            \n",
    "            pmod_values = trial_df[pmod].copy()\n",
    "            \n",
    "            for operation in ensure_list(spm_model.get(\"pmod_options\", [])):\n",
    "                \n",
    "                if operation == \"rank\":\n",
    "                    pmod_values = pmod_values.rank()\n",
    "                elif operation == \"minmax_scale\":\n",
    "                    pmod_values = (pmod_values - pmod_values.min()) / (pmod_values.max() - pmod_values.min())\n",
    "                elif operation == \"zscore\":\n",
    "                    pmod_values = (pmod_values - pmod_values.mean()) / pmod_values.std()\n",
    "                elif operation == \"fillna\":\n",
    "                    pmod_values = pmod_values.fillna(pmod_values.mean())\n",
    "            \n",
    "            event_df.loc[trial_df.index, pmod] = pmod_values.tolist()\n",
    "        \n",
    "        pmod_isna = trial_df[ensure_list(pmods)].isna().any(axis=1)\n",
    "        for idx, row in trial_df[pmod_isna].iterrows():\n",
    "            event_df.loc[idx, 'trial_type'] = trial_type + '_nopmod'\n",
    "        \n",
    "    (event_df[['onset', 'duration', 'trial_type'] + np.unique(all_pmods).tolist()]\n",
    "         .sort_values(by='onset')\n",
    "         .to_csv(os.path.join(env['data_path'], output_path, os.path.basename(event_path)), sep='\\t', index=False))\n",
    "    \n",
    "    return os.path.join(output_path, os.path.basename(event_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create job directory \n",
    "spm_model = model['SpecifySPMModel']\n",
    "env['job_path'] = ensure_relative(env['job_path'])\n",
    "\n",
    "job_path = os.path.join(env['data_path'], env['job_path'], job_name, 'jobs') \n",
    "os.makedirs(job_path, exist_ok=True)\n",
    "\n",
    "# Create outlier directory if outliers \n",
    "if spm_model.get(\"outlier\"):\n",
    "    outlier_path = os.path.join(env['job_path'], job_name, 'outlier') \n",
    "    os.makedirs(os.path.join(env['data_path'], outlier_path), exist_ok=True)\n",
    "\n",
    "# Check regressors are in .txt or .tsv files\n",
    "for regressors_path in ensure_list(spm_model.get('regressors', [])):\n",
    "    if (not regressors_path.endswith('txt')) and (not regressors_path.endswith('tsv')):\n",
    "        raise Exception(\"Regressors: only TSV or TXT file supported.\")\n",
    "\n",
    "regressor_names = ensure_list(spm_model.get('regressor_names',[]))\n",
    " \n",
    "## Create jobs for each sub   \n",
    "for sub in subs:\n",
    "    print(sub, \": \", end=\"\")\n",
    "    \n",
    "    job = copy.deepcopy(model)\n",
    "    \n",
    "    # Used to define path for functional runs, event files, and regressors\n",
    "    format_args = {}\n",
    "    format_args['sub'] = sub\n",
    "    format_args['task'] = task\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Remove excluded runs from job (model['Info']['exclude'])\n",
    "    runs = ensure_list(job['Info'].get('run',-1))\n",
    "    for exclude_sub, exclude_runs in job['Info'].get('exclude',{}).get('run',{}).items():\n",
    "        exclude_sub = exclude_sub.replace('sub-','')\n",
    "        if sub == exclude_sub:\n",
    "            for exclude_run in ensure_list(exclude_runs):\n",
    "                if exclude_run in runs:\n",
    "                    runs.remove(exclude_run)\n",
    "\n",
    "    # Add included runs from job (model['Info']['include'])\n",
    "    for include_sub, include_runs in job['Info'].get('include',{}).get('run',{}).items():\n",
    "        if sub == include_sub:\n",
    "            runs = runs + ensure_list(include_runs)\n",
    "\n",
    "    functional_runs = []\n",
    "    regressors = []\n",
    "    event_files = []\n",
    "    outlier_files = []\n",
    "\n",
    "    for run in runs:\n",
    "\n",
    "        if run != -1:\n",
    "            format_args['run'] = run\n",
    "        \n",
    "        # Add files of functional runs and regressors\n",
    "        func_path = ensure_exist(env['data_path'], spm_model['functional_runs'].format(**format_args))\n",
    "        functional_runs.append(func_path)\n",
    "            \n",
    "        regressors_path = ensure_exist(env['data_path'], spm_model['regressors'].format(**format_args))\n",
    "        regressors.append(regressors_path)\n",
    "        \n",
    "        # Checks regressor names exist in regressor file\n",
    "        if regressors_path.endswith('tsv'):\n",
    "            \n",
    "            regressor_df = pd.read_csv(os.path.join(env['data_path'], regressors_path), sep='\\t')\n",
    "\n",
    "            chosen_regressor_names = []\n",
    "            \n",
    "            col_exists = regressor_df.columns.isin(regressor_names)\n",
    "            if col_exists.sum() == 0:\n",
    "                raise Exception(\"No regressors found.\")\n",
    "                \n",
    "            chosen_regressor_names = regressor_df.columns[col_exists].tolist()\n",
    "            if col_exists.sum() != len(regressor_names):\n",
    "                issues.append(f\"Only some of the regressors are found: {chosen_regressor_names}\")\n",
    "\n",
    "        # Checks outlier names exist in regressor file \n",
    "        ## CHECK: is regressor_df created if path isn't .tsv?\n",
    "        if spm_model.get(\"outlier\"):\n",
    "            \n",
    "            outlier_indices = list(range(spm_model['outlier'].get(\"dummy_scan\", 0)))\n",
    "            \n",
    "            # Get Outliers by regressor name\n",
    "            #What is this exactly?\n",
    "            if spm_model[\"outlier\"].get(\"regressor_names\"):\n",
    "                \n",
    "                outlier_names = ensure_list(spm_model[\"outlier\"][\"regressor_names\"])\n",
    "                                \n",
    "                col_exists = regressor_df.columns.isin(outlier_names)\n",
    "                if col_exists.sum() > 0:\n",
    "                    chosen_outlier_names = regressor_df.columns[col_exists].tolist()\n",
    "                    outlier_indices = outlier_indices + list(np.ravel(np.where((regressor_df[chosen_outlier_names] != 0).any(axis=1))))\n",
    "                \n",
    "                    if col_exists.sum() != len(outlier_names):\n",
    "                        issues.append(f\"Only some of the outlier variables are found: {chosen_regressor_names}\")\n",
    "\n",
    "                else:\n",
    "                    issues.append(f\"No outlier variables found.\")\n",
    "            \n",
    "            outlier_indices = np.unique(np.array(outlier_indices, dtype=int))\n",
    "            \n",
    "            if len(outlier_indices) > 0:\n",
    "                \n",
    "                if run != -1:\n",
    "                    outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_run-{run}_outliers.txt\")\n",
    "                else:\n",
    "                    outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_outliers.txt\")\n",
    "\n",
    "                outlier_files.append(outlier_file)               \n",
    "                np.savetxt(os.path.join(env['data_path'], outlier_file), outlier_indices, fmt=\"%d\")\n",
    "        \n",
    "        # Get outlier by filename\n",
    "        elif spm_model.get(\"outlier_files\"):\n",
    "            \n",
    "            outlier_files.append(ensure_exist(env['data_path'], spm_model['outlier_files'].format(**format_args)))\n",
    "        \n",
    "        ## Read event file\n",
    "        event_path = ensure_exist(env['data_path'], spm_model['event_files'].format(**format_args))\n",
    "        event_df = pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "        \n",
    "        event_clip = False\n",
    "        tr_max = (nib.load(os.path.join(env['data_path'], func_path)).header.get_data_shape()[-1] - 1) * job[\"Info\"][\"tr\"]\n",
    "        \n",
    "        # Compare event times with total scan time to check if any events need to be discarded\n",
    "        if (event_df.onset.min() < 0):\n",
    "            issues.append(\"Events with negative onset time will be discarded.\")\n",
    "            event_clip = True\n",
    "        \n",
    "        if (event_df.onset.max() > tr_max):\n",
    "            issues.append(\"Events with onset time longer than scan length will be discarded.\")\n",
    "            event_clip = True\n",
    "        \n",
    "        # Add pmods to model\n",
    "        pmod_isna = False\n",
    "        \n",
    "        if spm_model.get(\"pmod\"):\n",
    "            \n",
    "            for trial_type, pmods in spm_model['pmod'].items():\n",
    "                trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "                for pmod in ensure_list(pmods):\n",
    "                    if trial_df[pmod].isna().any():\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has missing values for '{trial_type}' event.\")\n",
    "                        pmod_isna = True\n",
    "                    if trial_df[pmod].var() == 0:\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has zero variance for '{trial_type}' event, and will be excluded during analysis.\")\n",
    "        \n",
    "        # Add event files to model\n",
    "        if (spm_model.get(\"event_options\") is None) and (spm_model.get(\"pmod_options\") is None) and (pmod_isna == False) and (event_clip == False):\n",
    "            event_files.append(event_path)\n",
    "        else:            \n",
    "            event_files.append(customize_event(env, job_name, spm_model, event_path, tr_max))\n",
    "    \n",
    "    # Define SPMModel for job\n",
    "    job['SpecifySPMModel']['functional_runs'] = functional_runs\n",
    "    job['SpecifySPMModel']['event_files'] = event_files\n",
    "    job['SpecifySPMModel']['regressors'] = regressors\n",
    "    job['SpecifySPMModel']['regressor_names'] = chosen_regressor_names\n",
    "    \n",
    "    if len(outlier_files) > 0:\n",
    "        job['SpecifySPMModel']['outlier_files'] = outlier_files\n",
    "    \n",
    "    job['SpecifySPMModel']['time_repetition'] = job[\"Info\"][\"tr\"]\n",
    "    job['Level1Design']['interscan_interval'] = job[\"Info\"][\"tr\"]\n",
    "    \n",
    "    for key in [\"event_options\", \"pmod_options\", \"outlier\"]:\n",
    "        remove_key(key, job[\"SpecifySPMModel\"])\n",
    "\n",
    "    all_events = pd.concat([pd.read_csv(os.path.join(env['data_path'], x), sep='\\t') for x in job['SpecifySPMModel']['event_files']], \n",
    "                           ignore_index=True)\n",
    "    all_trial_types = all_events['trial_type'].unique()\n",
    "\n",
    "    # Automatically generate basic contrasts\n",
    "    if job.get('EstimateContrast', {}).get('basic_contrasts', False):\n",
    "        \n",
    "        contrasts = [] \n",
    "                               \n",
    "        for trial_type in all_trial_types:\n",
    "            contrasts.append([trial_type, \"T\", [trial_type], [1]])\n",
    "        \n",
    "        for cond, pmods in spm_model.get('pmod',{}).items():\n",
    "            for pmod in ensure_list(pmods):\n",
    "                contrasts.append([f'{cond}x{pmod}^1', \"T\", [f'{cond}x{pmod}^1'], [1]])\n",
    "                \n",
    "        contrasts = contrasts + ensure_list(job['EstimateContrast'].get('contrasts',[]))\n",
    "            \n",
    "        job['EstimateContrast']['contrasts'] = contrasts\n",
    "        del job['EstimateContrast']['basic_contrasts']\n",
    "    \n",
    "    removed_contrast = []\n",
    "    \n",
    "    # Manually define contrasts\n",
    "    for contrast in job.get('EstimateContrast', {}).get('contrasts', []):\n",
    "        \n",
    "        contrast_available = True\n",
    "        \n",
    "        for trial_type in contrast[2]:\n",
    "            \n",
    "            if trial_type.endswith('^1'):\n",
    "                trial_type = trial_type.split('x')[0]\n",
    "            \n",
    "            if trial_type not in all_trial_types:\n",
    "                contrast_available = False\n",
    "                \n",
    "        if not contrast_available:\n",
    "            removed_contrast.append(contrast)\n",
    "            \n",
    "    for rc in removed_contrast:\n",
    "        job['EstimateContrast']['contrasts'].remove(rc)\n",
    "    \n",
    "    if len(removed_contrast) > 0:\n",
    "        issues.append(\"These contrasts have been removed: \" + str([rc[0] for rc in removed_contrast]))\n",
    "    \n",
    "    job[\"Info\"][\"sub\"] = sub\n",
    "\n",
    "    for key in [\"job_path\"]:\n",
    "        remove_key(key, job[\"Environment\"])\n",
    "\n",
    "    for key in [\"sub_container\", \"exclude\", \"run\", \"tr\"]:\n",
    "        remove_key(key, job[\"Info\"])\n",
    "\n",
    "    job_output = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    with open(job_output, 'w') as f:\n",
    "        json.dump(job, f)\n",
    "        \n",
    "    if len(issues) == 0:\n",
    "        print(\"job created\")\n",
    "    else:\n",
    "        print(\"issues found - \\n\\t\" + \"\\n\\t\".join(issues))\n",
    "        \n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_path = os.path.join(env['data_path'], env['job_path'], job_name, 'slurm') \n",
    "os.makedirs(slurm_path, exist_ok=True)\n",
    "try:\n",
    "    os.chmod(slurm_path, 0o0777)\n",
    "    \n",
    "    os.makedirs(os.path.join(slurm_path, 'out'), exist_ok=True)\n",
    "    os.chmod(os.path.join(slurm_path, 'out'), 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "for sub in subs:\n",
    "    \n",
    "    slurm_header = []\n",
    "    slurm_header.append(\"#!/bin/bash\")\n",
    "    slurm_header.append(f\"#SBATCH --job-name=sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --output=out/sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --error=out/sub-{sub}.err\")\n",
    "    slurm_header.append(\"#SBATCH --time=5-00:00\")\n",
    "    slurm_header.append(\"\")\n",
    "    slurm_header.append(\"srun \")\n",
    "\n",
    "    data_path = env['data_path']\n",
    "    output_path = env['output_path']\n",
    "    working_path = env['working_path']\n",
    "\n",
    "    json_path = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    \n",
    "    cmd = []\n",
    "    cmd.append(\"singularity run --cleanenv\")\n",
    "    cmd.append(f\"-B {script_path}:/worker.py\")\n",
    "    cmd.append(f\"-B {data_path}:/data\")\n",
    "    cmd.append(f\"-B {output_path}:/output\")\n",
    "    cmd.append(f\"-B {working_path}:/working\")\n",
    "    cmd.append(f\"-B {json_path}:/job.json\")\n",
    "    cmd.append(f\"{singularity_path} python /worker.py /job.json\")\n",
    "    \n",
    "    slurm_output = os.path.join(output_path, job_name, f\"sub-{sub}\")\n",
    "    slurm_working = os.path.join(working_path, job_name, f\"sub-{sub}\")\n",
    "    \n",
    "    slurm_footer = []\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_output}\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_working}\")\n",
    "    \n",
    "    slurm_cmd = \"\\n\".join(slurm_header) + \" \\\\\\n  \".join(cmd) + \"\\n\".join(slurm_footer) \n",
    "    \n",
    "    with open(os.path.join(slurm_path, f\"sub-{sub}.job\"), 'w') as f:\n",
    "        f.write(slurm_cmd)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DRIVE ONE MODEL (SLURM JOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test drive one job (sub-DP6), copy and paste the following lines in terminal:\n",
      "\n",
      "singularity run --cleanenv \\\n",
      "  -B /data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/l1analysis_SPM.py:/worker.py \\\n",
      "  -B /data00/projects/megameta/darpa1:/data \\\n",
      "  -B /data00/projects/megameta/darpa1/derivatives/nipype:/output \\\n",
      "  -B /data00/projects/megameta/darpa1/working/nipype:/working \\\n",
      "  -B /data00/projects/megameta/darpa1/models/task-share_model-beta/jobs/sub-DP6.json:/job.json \\\n",
      "  /data00/tools/singularity_images/neurodocker.sqsh python /worker.py /job.json\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(f\"To test drive one job (sub-{sub}), copy and paste the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\" \\\\\\n  \".join(cmd))\n",
    "\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN ALL MODELS (SLURM JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\n",
      "\n",
      "cd /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP111.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP113.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP121.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP122.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP147.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP157.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP186.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP21.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP210.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP248.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP251.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP259.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP33.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP332.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP34.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP350.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP357.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP362.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP381.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP386.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP398.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP410.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP414.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP420.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP426.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP429.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP479.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP50.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP509.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP573.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP616.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP643.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP69.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP77.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP78.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP808.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP787.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP792.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP701.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP757.job\n",
      "sbatch -D /data00/projects/megameta/darpa1/models/task-share_model-beta/slurm -c 8 sub-DP6.job\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"cd {slurm_path}\")\n",
    "for sub in subs:\n",
    "    print(f\"sbatch -D {slurm_path} -c 8 sub-{sub}.job\")\n",
    "print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
