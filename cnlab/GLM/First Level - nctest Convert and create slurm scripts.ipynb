{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Plug in the `model_path` (and change `script_path` and `singularity_path` if neccessary).\n",
    "2. From `Run` menu, select `Restart Kernel and Run All Cells`\n",
    "3. From `View` menu, select `Collapse All Code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data00/projects/megameta/scripts/jupyter_megameta/first_level_models/BA/model_specifications/task-walkstatement_model-messageUpdateTest.json'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = '/data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/first_level_HY.py'\n",
    "singularity_path = '/data00/tools/singularity_images/neurodocker.sqsh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, copy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_exist(path, file):\n",
    "    if os.path.exists(os.path.join(path, file)):\n",
    "        return file\n",
    "    else:\n",
    "        raise Exception(\"File missing: \" + file)\n",
    "\n",
    "def ensure_relative(path):\n",
    "    if path.startswith('/'):\n",
    "        raise Exception(\"Make sure path is relative to the data path: \" + path)\n",
    "    else:\n",
    "        return path\n",
    "        \n",
    "def ensure_list(obj):\n",
    "    if type(obj) is not list:\n",
    "        return [obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def remove_key(key, obj):\n",
    "    if obj.get(key):\n",
    "        del obj[key]\n",
    "        \n",
    "def copy_from_template(target, template):\n",
    "    for key, item in template.items():\n",
    "        if key not in target.keys():\n",
    "            target[key] = copy.deepcopy(template[key])\n",
    "        elif type(item) is dict:\n",
    "            copy_from_template(target[key], template[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template megameta_default.json: Mega-meta default pipeline\n",
      "\t6mm FWHM smoothing\n",
      "\tNo global scaling\n",
      "\tFAST correlation\n",
      "\t6 motion parameters and framewise displacement\n",
      "\tOutlier: FD > 0.75\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, 'r') as f:\n",
    "    model = json.load(f)\n",
    "    \n",
    "if model.get(\"Template\"):\n",
    "\n",
    "    template_path = model.get(\"Template\")\n",
    "    with open(template_path, 'r') as f:\n",
    "        template = json.load(f)\n",
    "    \n",
    "    print(f\"Using template {os.path.basename(template_path)}: \", end=\"\")\n",
    "    print(\"\\n\\t\".join(template.get(\"Descriptions\", [\"No description found\"])))\n",
    "\n",
    "    copy_from_template(model, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 34 participants:\n",
      "['BA224', 'BA227', 'BA228', 'BA229', 'BA234', 'BA235', 'BA237', 'BA238', 'BA239', 'BA240', 'BA242', 'BA244', 'BA247', 'BA250', 'BA251', 'BA252', 'BA253', 'BA255', 'BA257', 'BA258', 'BA261', 'BA262', 'BA263', 'BA265', 'BA266', 'BA267', 'BA269', 'BA272', 'BA273', 'BA275', 'BA276', 'BA277', 'BA278', 'BA245']\n"
     ]
    }
   ],
   "source": [
    "task = model['Info']['task']\n",
    "job_name = 'task-{}_model-{}'.format(model['Info']['task'], model['Info']['model'])\n",
    "\n",
    "env = model['Environment']\n",
    "\n",
    "os.makedirs(env['output_path'], exist_ok=True)\n",
    "os.makedirs(env['working_path'], exist_ok=True)\n",
    "\n",
    "try:\n",
    "    os.chmod(env['output_path'], 0o0777)\n",
    "    os.chmod(env['working_path'], 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "subs = []\n",
    "if model['Info'].get('sub'):\n",
    "    subs = ensure_list(model['Info']['sub'])\n",
    "elif model['Info'].get('sub_container'):\n",
    "    sub_container = ensure_relative(model['Info']['sub_container'])\n",
    "    subs = [x.split(os.path.sep)[-2].replace('sub-','') for x in glob.glob(os.path.join(env['data_path'], sub_container, 'sub-*'+os.path.sep))]\n",
    "\n",
    "exclude_subs = model['Info'].get('exclude',{}).get('sub',[])\n",
    "for es in exclude_subs:\n",
    "    if es in subs:\n",
    "        subs.remove(es)\n",
    "\n",
    "if len(subs) == 0:\n",
    "    raise Exception(\"No subjects found.\")\n",
    "else:\n",
    "    print(f\"Processing {len(subs)} participants:\")\n",
    "    print(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_event(env, job_name, spm_model, event_path):\n",
    "    \n",
    "    output_path = os.path.join(ensure_relative(env['job_path']), job_name, 'events') \n",
    "    event_df = pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "    \n",
    "    for operation, params in spm_model.get(\"event_options\", {}).items():\n",
    "        \n",
    "        if operation == 'map_condition':\n",
    "            \n",
    "            condition_map = {}\n",
    "            for trial_type in event_df['trial_type'].unique():\n",
    "                condition_map[trial_type] = trial_type\n",
    "                \n",
    "            for new_trial_type, trial_types in params.items():\n",
    "                for trial_type in ensure_list(trial_types):\n",
    "                    condition_map[trial_type] = new_trial_type\n",
    "            \n",
    "            event_df['trial_type'] = event_df['trial_type'].map(condition_map)\n",
    "            \n",
    "        elif operation == 'melt_condition':\n",
    "            \n",
    "            for trial_type, trial_data in params.items():\n",
    "                for idx, row in event_df.query(f'trial_type == \"{trial_type}\"').iterrows():\n",
    "                    event_df.loc[idx, 'trial_type'] = trial_type + \"_\" + str(row[trial_data])\n",
    "\n",
    "    all_pmods = []\n",
    "    for trial_type, pmods in spm_model.get('pmod',{}).items():\n",
    "        \n",
    "        trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "        for pmod in ensure_list(pmods):\n",
    "            all_pmods.append(pmod)\n",
    "            \n",
    "            pmod_values = trial_df[pmod].copy()\n",
    "            pmod_values = pmod_values.fillna(pmod_values.mean())\n",
    "            \n",
    "            for operation in ensure_list(spm_model.get(\"pmod_options\", [])):\n",
    "                \n",
    "                if operation == \"rank\":\n",
    "                    pmod_values = pmod_values.rank()\n",
    "                elif operation == \"minmax_scale\":\n",
    "                    pmod_values = (pmod_values - pmod_values.min()) / (pmod_values.max() - pmod_values.min())\n",
    "                elif operation == \"zscore\":\n",
    "                    pmod_values = (pmod_values - pmod_values.mean()) / pmod_values.std()\n",
    "            \n",
    "            event_df.loc[trial_df.index, pmod] = pmod_values.tolist()\n",
    "        \n",
    "    (event_df[['onset', 'duration', 'trial_type'] + np.unique(all_pmods).tolist()]\n",
    "         .sort_values(by='onset')\n",
    "         .to_csv(os.path.join(env['data_path'], output_path, os.path.basename(event_path)), sep='\\t', index=False))\n",
    "    \n",
    "    return os.path.join(output_path, os.path.basename(event_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA224 : job created\n",
      "BA227 : job created\n",
      "BA228 : job created\n",
      "BA229 : job created\n",
      "BA234 : job created\n",
      "BA235 : job created\n",
      "BA237 : job created\n",
      "BA238 : job created\n",
      "BA239 : job created\n",
      "BA240 : job created\n",
      "BA242 : job created\n",
      "BA244 : job created\n",
      "BA247 : job created\n",
      "BA250 : job created\n",
      "BA251 : job created\n",
      "BA252 : job created\n",
      "BA253 : job created\n",
      "BA255 : job created\n",
      "BA257 : job created\n",
      "BA258 : job created\n",
      "BA261 : job created\n",
      "BA262 : job created\n",
      "BA263 : job created\n",
      "BA265 : job created\n",
      "BA266 : job created\n",
      "BA267 : job created\n",
      "BA269 : job created\n",
      "BA272 : job created\n",
      "BA273 : job created\n",
      "BA275 : job created\n",
      "BA276 : job created\n",
      "BA277 : job created\n",
      "BA278 : job created\n",
      "BA245 : job created\n"
     ]
    }
   ],
   "source": [
    "spm_model = model['SpecifySPMModel']\n",
    "\n",
    "env['job_path'] = ensure_relative(env['job_path'])\n",
    "\n",
    "job_path = os.path.join(env['data_path'], env['job_path'], job_name, 'jobs') \n",
    "os.makedirs(job_path, exist_ok=True)\n",
    "\n",
    "if (spm_model.get(\"event_options\") is not None) or (spm_model.get(\"pmod_options\") is not None):\n",
    "    event_path = os.path.join(env['job_path'], job_name, 'events') \n",
    "    os.makedirs(os.path.join(env['data_path'], event_path), exist_ok=True)\n",
    "\n",
    "if spm_model.get(\"outlier\"):\n",
    "    outlier_path = os.path.join(env['job_path'], job_name, 'outlier') \n",
    "    os.makedirs(os.path.join(env['data_path'], outlier_path), exist_ok=True)\n",
    "    \n",
    "for regressors_path in ensure_list(spm_model.get('regressors', [])):\n",
    "    if (not regressors_path.endswith('txt')) and (not regressors_path.endswith('tsv')):\n",
    "        raise Exception(\"Regressors: only TSV or TXT file supported.\")\n",
    "\n",
    "regressor_names = ensure_list(spm_model.get('regressor_names',[[]]))\n",
    "if type(regressor_names[0]) is not list:\n",
    "    regressor_names = [regressor_names]\n",
    "    \n",
    "for sub in subs:\n",
    "    print(sub, \": \", end=\"\")\n",
    "    \n",
    "    job = copy.deepcopy(model)\n",
    "    \n",
    "    format_args = {}\n",
    "    format_args['sub'] = sub\n",
    "    format_args['task'] = task\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    runs = ensure_list(job['Info'].get('run',-1))\n",
    "    for exclude_sub, exclude_runs in job['Info'].get('exclude',{}).get('run',{}).items():\n",
    "        if sub == exclude_sub:\n",
    "            for exclude_run in ensure_list(exclude_runs):\n",
    "                if exclude_run in runs:\n",
    "                    runs.remove(exclude_run)\n",
    "                    \n",
    "    functional_runs = []\n",
    "    regressors = []\n",
    "    event_files = []\n",
    "    outlier_files = []\n",
    "\n",
    "    for run in runs:\n",
    "        \n",
    "        if run != -1:\n",
    "            format_args['run'] = run\n",
    "        \n",
    "        functional_runs.append(ensure_exist(env['data_path'], spm_model['functional_runs'].format(**format_args)))\n",
    "            \n",
    "        regressors_path = ensure_exist(env['data_path'], spm_model['regressors'].format(**format_args))\n",
    "        regressors.append(regressors_path)\n",
    "\n",
    "        if regressors_path.endswith('tsv'):\n",
    "            \n",
    "            regressor_df = pd.read_csv(os.path.join(env['data_path'], regressors_path), sep='\\t')\n",
    "\n",
    "            chosen_regressor_names = []\n",
    "            for rn in regressor_names:\n",
    "                col_exists = regressor_df.columns.isin(rn)\n",
    "                if col_exists.sum() > 0:\n",
    "                    chosen_regressor_names = regressor_df.columns[col_exists].tolist()\n",
    "                    if col_exists.sum() != len(rn):\n",
    "                        issues.append(f\"{regressors_path}: Only some of the regressors are found - {chosen_regressor_names}\")\n",
    "                    break\n",
    "\n",
    "            if len(chosen_regressor_names) == 0:\n",
    "                raise Exception(\"No regressors found.\")\n",
    "\n",
    "        #if spm_model.get(\"outlier\"):\n",
    "        #    \n",
    "        #    outlier_indices = list(range(spm_model['outlier'].get(\"dummy_scan\", 0)))\n",
    "        #    \n",
    "        #    if spm_model[\"outlier\"].get(\"regressor_names\"):\n",
    "        #        \n",
    "        #        outlier_names = ensure_list(spm_model[\"outlier\"][\"regressor_names\"])\n",
    "        #        \n",
    "        #        col_exists = regressor_df.columns.isin(outlier_names)\n",
    "        #        if col_exists.sum() > 0:\n",
    "        #            chosen_outlier_names = regressor_df.columns[col_exists].tolist()\n",
    "        #            outlier_indices = outlier_indices + list(np.ravel(np.where((regressor_df[chosen_outlier_names] != 0).any(axis=1))))\n",
    "        #        else:\n",
    "        #            raise Exception(\"No outliers found.\")\n",
    "\n",
    "        #    outlier_indices = np.unique(np.array(outlier_indices, dtype=int))\n",
    "        #    \n",
    "        #    if run != -1:\n",
    "        #        outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_run-{run}_outliers.txt\")\n",
    "        #    else:\n",
    "        #        outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_outliers.txt\")\n",
    "        #        \n",
    "        #    outlier_files.append(outlier_file)               \n",
    "        #    np.savetxt(os.path.join(env['data_path'], outlier_file), outlier_indices, fmt=\"%d\")\n",
    "            \n",
    "        event_path = ensure_exist(env['data_path'], spm_model['event_files'].format(**format_args))\n",
    "\n",
    "        if spm_model.get(\"pmod\"):\n",
    "            event_df = pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "            \n",
    "            for trial_type, pmods in spm_model['pmod'].items():\n",
    "                trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "                for pmod in ensure_list(pmods):\n",
    "                    if trial_df[pmod].isna().any():\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has missing values for '{trial_type}' events. They will be replaced by mean.\")                        \n",
    "                    if trial_df[pmod].var() == 0:\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has zero variance for '{trial_type}' events, and will cause error. Consider excluding this run.\")\n",
    "        \n",
    "        if (spm_model.get(\"event_options\") is None) and (spm_model.get(\"pmod_options\") is None):\n",
    "            event_files.append(event)\n",
    "        else:\n",
    "            event_files.append(customize_event(env, job_name, spm_model, event_path))\n",
    "                \n",
    "    job['SpecifySPMModel']['functional_runs'] = functional_runs\n",
    "    job['SpecifySPMModel']['event_files'] = event_files\n",
    "    job['SpecifySPMModel']['regressors'] = regressors\n",
    "    job['SpecifySPMModel']['regressor_names'] = chosen_regressor_names\n",
    "    \n",
    "    if len(outlier_files) > 0:\n",
    "        job['SpecifySPMModel']['outlier_files'] = outlier_files\n",
    "    \n",
    "    job['SpecifySPMModel']['time_repetition'] = job[\"Info\"][\"tr\"]\n",
    "    job['Level1Design']['interscan_interval'] = job[\"Info\"][\"tr\"]\n",
    "    \n",
    "    for key in [\"event_options\", \"pmod_options\", \"outlier\"]:\n",
    "        remove_key(key, job[\"SpecifySPMModel\"])\n",
    "\n",
    "    if job['EstimateContrast'].get('basic_contrasts', False):\n",
    "        all_events = pd.concat([pd.read_csv(os.path.join(env['data_path'], x), sep='\\t') for x in job['SpecifySPMModel']['event_files']], \n",
    "                               ignore_index=True)\n",
    "        \n",
    "        contrasts = [] \n",
    "                               \n",
    "        for trial_type in all_events['trial_type'].unique():\n",
    "            contrasts.append([trial_type, \"T\", [trial_type], [1]])\n",
    "        \n",
    "        for cond, pmods in spm_model.get('pmod',{}).items():\n",
    "            for pmod in ensure_list(pmods):\n",
    "                contrasts.append([f'{cond}x{pmod}^1', \"T\", [f'{cond}x{pmod}^1'], [1]])\n",
    "                \n",
    "        contrasts = contrasts + ensure_list(job['EstimateContrast'].get('contrasts',[]))\n",
    "            \n",
    "        job['EstimateContrast']['contrasts'] = contrasts\n",
    "        del job['EstimateContrast']['basic_contrasts']\n",
    "    \n",
    "    job[\"Info\"][\"sub\"] = sub\n",
    "\n",
    "    for key in [\"job_path\"]:\n",
    "        remove_key(key, job[\"Environment\"])\n",
    "\n",
    "    for key in [\"sub_container\", \"exclude\", \"run\", \"tr\"]:\n",
    "        remove_key(key, job[\"Info\"])\n",
    "                \n",
    "    job_output = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    with open(job_output, 'w') as f:\n",
    "        json.dump(job, f)\n",
    "        \n",
    "    if len(issues) == 0:\n",
    "        print(\"job created\")\n",
    "    else:\n",
    "        print(\"issues found - \\n\\t\" + \"\\n\\t\".join(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_path = os.path.join(env['data_path'], env['job_path'], job_name, 'slurm') \n",
    "os.makedirs(slurm_path, exist_ok=True)\n",
    "try:\n",
    "    os.chmod(slurm_path, 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "os.makedirs(os.path.join(slurm_path, 'out'), exist_ok=True)\n",
    "os.chmod(os.path.join(slurm_path, 'out'), 0o0777)\n",
    "\n",
    "for sub in subs:\n",
    "    \n",
    "    slurm_header = []\n",
    "    slurm_header.append(\"#!/bin/bash\")\n",
    "    slurm_header.append(f\"#SBATCH --job-name=sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --output=out/sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --error=out/sub-{sub}.err\")\n",
    "    slurm_header.append(\"#SBATCH --time=5-00:00\")\n",
    "    slurm_header.append(\"\")\n",
    "    slurm_header.append(\"srun \")\n",
    "\n",
    "    data_path = env['data_path']\n",
    "    output_path = env['output_path']\n",
    "    working_path = env['working_path']\n",
    "\n",
    "    json_path = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    \n",
    "    cmd = []\n",
    "    cmd.append(\"singularity run --cleanenv\")\n",
    "    cmd.append(f\"-B {script_path}:/worker.py\")\n",
    "    cmd.append(f\"-B {data_path}:/data\")\n",
    "    cmd.append(f\"-B {output_path}:/output\")\n",
    "    cmd.append(f\"-B {working_path}:/working\")\n",
    "    cmd.append(f\"-B {json_path}:/job.json\")\n",
    "    cmd.append(f\"{singularity_path} python /worker.py /job.json\")\n",
    "    \n",
    "    slurm_output = os.path.join(output_path, job_name, f\"sub-{sub}\")\n",
    "    slurm_working = os.path.join(working_path, job_name, f\"sub-{sub}\")\n",
    "    \n",
    "    slurm_footer = []\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_output}\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_working}\")\n",
    "    \n",
    "    slurm_cmd = \"\\n\".join(slurm_header) + \" \\\\\\n  \".join(cmd) + \"\\n\".join(slurm_footer) \n",
    "    \n",
    "    with open(os.path.join(slurm_path, f\"sub-{sub}.job\"), 'w') as f:\n",
    "        f.write(slurm_cmd)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test drive one job (sub-BA245), copy and paste the following lines in terminal:\n",
      "\n",
      "singularity run --cleanenv \\\n",
      "  -B /data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/first_level_HY.py:/worker.py \\\n",
      "  -B /data00/projects/megameta/BA:/data \\\n",
      "  -B /data00/projects/megameta/BA/derivatives/nipype:/output \\\n",
      "  -B /data00/projects/megameta/BA/working/nipype:/working \\\n",
      "  -B /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/jobs/sub-BA245.json:/job.json \\\n",
      "  /data00/tools/singularity_images/neurodocker.sqsh python /worker.py /job.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"To test drive one job (sub-{sub}), copy and paste the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\" \\\\\\n  \".join(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\n",
      "\n",
      "cd /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA224.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA227.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA228.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA229.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA234.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA235.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA237.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA238.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA239.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA240.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA242.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA244.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA247.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA250.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA251.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA252.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA253.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA255.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA257.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA258.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA261.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA262.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA263.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA265.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA266.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA267.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA269.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA272.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA273.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA275.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA276.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA277.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA278.job\n",
      "sbatch -D /data00/projects/megameta/BA/models/task-walkstatement_model-messageUpdateTest/slurm -c 8 sub-BA245.job\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"cd {slurm_path}\")\n",
    "for sub in subs:\n",
    "    print(f\"sbatch -D {slurm_path} -c 8 sub-{sub}.job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
