{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building first level models using _nipype_ and _SPM12_\n",
    "\n",
    "## Base functionality for _megameta_ project\n",
    "\n",
    "-------\n",
    "#### History\n",
    "\n",
    "* 4/16/19 cscholz - making changes to read in Banners and Image tasks of P1 as separate runs\n",
    "* 4/9/19 cscholz - made small correction to make_contrast_list() (setting: -1/neg_length instead of -1/pos_length)\n",
    "* 4/2/19 mbod - split out processing pipeline for revised workflow\n",
    "* 3/28/19 mbod - update pipeline to include resampling to template & SPM path reference\n",
    "* 3/23/19 mbod - include contrast definition in the config JSON file\n",
    "* 3/9/19 mbod - updates from testing template with `darpa1`\n",
    "* 2/27/19 mbod  - modify example notebook to make base functionality notebook\n",
    "\n",
    "-----\n",
    "\n",
    "### Description\n",
    "\n",
    "* Set up a nipype workflow to use SPM12 to make first level models for _megameta_ task data (preprocessed using `batch8` SPM8 scripts) in BIDS derivative format   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### Template variables\n",
    "\n",
    "* Specify the following values:\n",
    "    1. project name - should be name of folder under `/data00/project/megameta`, e.g. `project1`\n",
    "    2. filename for JSON model specification (should be inside `model_specification` folder), e.g. `p1_image_pmod_likeme.json`\n",
    "    3. TR value in seconds\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### Setup\n",
    "\n",
    "* import required modules and define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # system functions\n",
    "\n",
    "# NIYPE FUNCTIONS\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.interfaces.spm as spm          # spm\n",
    "import nipype.interfaces.matlab as mlab      # how to run matlab\n",
    "import nipype.interfaces.utility as util     # utility\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from nilearn import plotting, image\n",
    "from nistats import thresholding\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NC commented out 04/19/19\n",
    "#MODEL_SPEC_FILE = 'project1_model_BANNER_IMAGE_pmod_pop_rank.json'\n",
    "#PROJECT_NAME = 'project1'\n",
    "#TR=2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matlab path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the way matlab should be called\n",
    "mlab.MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "# If SPM is not in your MATLAB path you should add it here\n",
    "mlab.MatlabCommand.set_default_paths(PATH_TO_SPM_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* These need to be reformatted to be consistent\n",
    "* as data is not smoothed commented out the `fwhm_size` param - but data probably has a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_MODEL_FILE = os.path.join('/data00/projects/megameta/scripts/jupyter_megameta/first_level_models',\n",
    "                               PROJECT_NAME, 'model_specifications',\n",
    "                               MODEL_SPEC_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(JSON_MODEL_FILE) as fh:\n",
    "    model_def = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cscholz: Using a task pattern instead of a simple name to read in 2 tasks\n",
    "TASK_NAME = model_def['TaskName']\n",
    "RUNS = model_def['Runs']\n",
    "MODEL_NAME = model_def['ModelName']\n",
    "PROJECT_NAME = model_def['ProjectID']\n",
    "\n",
    "TASK_PATTERN = \"[ib][am]*\"   # hack for combined model look for task names that have first two characters \"im\" or \"ba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join('/data00/projects/megameta', PROJECT_NAME)\n",
    "SUBJ_DIR = os.path.join(PROJECT_DIR, 'derivatives', 'nipype', 'resampled_and_smoothed')\n",
    "\n",
    "\n",
    "task_func_template = \"sr{PID}_task-{TASK}_run-0{RUN}_space-MNI152-T1-1mm_desc-preproc_bold.nii\"\n",
    "\n",
    "\n",
    "subject_list = [subj for subj in os.listdir(SUBJ_DIR) \n",
    "                   if os.path.exists(os.path.join(SUBJ_DIR,subj,'medium', 'fwhm_8',\n",
    "                                        task_func_template.format(PID=subj, TASK=TASK_NAME[0], RUN=1)))\n",
    "                   or os.path.exists(os.path.join(SUBJ_DIR,subj,'medium', 'fwhm_8',\n",
    "                                        task_func_template.format(PID=subj, TASK=TASK_NAME[1], RUN=1)))]\n",
    "\n",
    "output_dir = os.path.join(PROJECT_DIR,'derivatives', 'nipype','model_{}_{}_{}'.format(TASK_NAME[0].upper(),TASK_NAME[1].upper(), MODEL_NAME))        # name of 1st-level output folder\n",
    "working_dir = os.path.join(PROJECT_DIR, 'working', \n",
    "                           'nipype', 'workingdir_model_{}_{}_{}'.format(TASK_NAME[0].upper(),TASK_NAME[1].upper(), MODEL_NAME))   # name of 1st-level working directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check to see if output and work directories exist\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir) \n",
    "\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    subject_list = [ s for s in subject_list if s not in exclude_subjects ]\n",
    "    print('\\n\\nApplied subject inclusion list:\\n\\t',' '.join(exclude_subjects))\n",
    "except:\n",
    "    print('\\n\\nNo subject exclusions applied')\n",
    "\n",
    "try:\n",
    "    subject_list = [ s for s in subject_list if s in include_subjects ]\n",
    "    print('\\n\\nApplied subject inclusion list:\\n\\t',' '.join(include_subjects))\n",
    "except:\n",
    "    print('\\n\\nNo subject inclusions applied')\n",
    "\n",
    "    \n",
    "print('\\n\\nSUBJECT LIST IS:\\n\\t', ' '.join(subject_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for subject info and contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup design matrix data for subject\n",
    "\n",
    "* need a function to set up the nipype `Bunch` format used\n",
    "    * https://nipype.readthedocs.io/en/latest/users/model_specification.html\n",
    "* read the onsets/dur/conditions from task logs and extract needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject_info(subject_id, model_path, DEBUG=False):\n",
    "    '''\n",
    "    1. load model specification from JSON spec file\n",
    "    2. get confound file for subject for task to add to design matrix     \n",
    "    3. get task spec CSV for subject for task\n",
    "    4. setup subject info structure\n",
    "    '''\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    from nipype.interfaces.base import Bunch\n",
    "    \n",
    "    \n",
    "    def make_pmod(df, conditions, pmods={}, normalize='mean'):\n",
    "        \n",
    "        pmod = []\n",
    "        \n",
    "        for cond in conditions:\n",
    "        \n",
    "            \n",
    "            if not pmods.get(cond):\n",
    "                pmod.append(None)\n",
    "            else:\n",
    "                df2 = df[df.trial_type==cond]\n",
    "                \n",
    "                pmod_name = pmods.get(cond)\n",
    "                \n",
    "                #pmod = [pmod] if not type(pmods) is list else pmod\n",
    "                \n",
    "                # MAKE SURE THERE IS VARIANCE IN PMOD VECTOR\n",
    "                if df2[pmod_name].var()==0:\n",
    "                    #df2[pmod_name]+=0.001\n",
    "                    pmod.append(None)\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # APPLY NORMALIZATION\n",
    "                if normalize=='mean':\n",
    "                    df2[pmod_name] = df2[pmod_name] - df2[pmod_name].mean()\n",
    "\n",
    "                pmod.append(Bunch(name=[pmod_name],\n",
    "                      param=[df2[pmod_name].values.tolist()\n",
    "                            ],\n",
    "                      poly=[1]\n",
    "                     ))\n",
    "        \n",
    "        return pmod\n",
    "    \n",
    "    \n",
    "\n",
    "    def map_spec_to_model(spec_df,model):\n",
    "        \"\"\"\n",
    "        Maps spec trial names to model contrast trials.\n",
    "\n",
    "        Args:\n",
    "            spec: the events.tsv spec file\n",
    "            model: the model.json file\n",
    "\n",
    "        Returns:\n",
    "            pandas dataframe object\n",
    "        \"\"\"\n",
    "\n",
    "        spec=spec_df.copy()\n",
    "\n",
    "        for con in model['Conditions']:\n",
    "            spec_trials = model['Conditions'][con]\n",
    "            spec.loc[spec.trial_type.isin(spec_trials),'trial_type'] = con\n",
    "            spec.onset.sort_values()\n",
    "\n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    with open(model_path) as fh:\n",
    "        model_def = json.load(fh)\n",
    "    \n",
    "    \n",
    "    pmod = None if not model_def.get('Modulators') else []\n",
    "    \n",
    "    TASK_NAME = model_def['TaskName']\n",
    "    TASK_RUNS = model_def['Runs']\n",
    "    MODEL_NAME = model_def['ModelName']\n",
    "    PROJECT_ID = model_def['ProjectID']\n",
    "    \n",
    "    #cscholz hack to allow for missing conditions per run\n",
    "    #condition_names = list(model_def['Conditions'].keys())\n",
    "   \n",
    "    \n",
    "    PROJECT_DIR = os.path.join('/data00/projects/megameta', PROJECT_ID)\n",
    "    SUBJ_DIR = os.path.join(PROJECT_DIR,'derivatives', 'batch8')\n",
    "    \n",
    "    realign_files = []\n",
    "    subject_info = []\n",
    "    \n",
    "\n",
    "    \n",
    "    # check to see which runs exist for subject\n",
    "    # by looking for appropriate events.tsv files\n",
    "    # this could (should?) also include looking for the nifti file?\n",
    "    ## CSCHOLZ: putting new runs_for_subj function to allow for multiple tasks\n",
    "    #runs_for_subj = [run for run in TASK_RUNS\n",
    "    #                if \n",
    "    #                os.path.exists(os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "    #                                '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "    #                                                                       TASK_NAME,\n",
    "    #                                                                       run)))\n",
    "    #                ]\n",
    "    runs_for_subj = {}\n",
    "    for task in TASK_RUNS:\n",
    "        cur_run_list= [run for run in TASK_RUNS[task]\n",
    "                       if \n",
    "                       os.path.exists(os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                    '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                           task,\n",
    "                                                                           run)))\n",
    "                      ]\n",
    "        runs_for_subj[task]=cur_run_list\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"runs_for_subj\", runs_for_subj)\n",
    "        print(\"checked paths:\")\n",
    "        for run in TASK_RUNS:\n",
    "            print('\\t', os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                    '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                           TASK_NAME,\n",
    "                                                                           run)))\n",
    "        print(\"TASK NAME\", TASK_NAME)\n",
    "        print(\"pmod\", pmod)\n",
    "        print(\"TASK_RUNS\", TASK_RUNS)\n",
    "        print(\"subject_id\", subject_id)\n",
    "    \n",
    "    # Cscholz looping through tasks and then through run per task to allow for multiple tasks\n",
    "    for task in runs_for_subj:\n",
    "        \n",
    "        for run_num, _ in enumerate(runs_for_subj[task],1):\n",
    "    \n",
    "            #cscholz adding hack to allow for missing conditions per run\n",
    "            condition_names=list(model_def['Conditions'].keys())\n",
    "            \n",
    "            events_df = pd.read_csv(os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                             '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                                    task,\n",
    "                                                                                    run_num)),\n",
    "                               sep='\\t')\n",
    "\n",
    "            #cscholz removing conditions not present this run\n",
    "            remove_conds=[]\n",
    "            for cond in condition_names:\n",
    "                present_conds=0\n",
    "                for orig_cond in model_def['Conditions'][cond]:\n",
    "                    if orig_cond in list(events_df['trial_type']):\n",
    "                        present_conds=present_conds+1\n",
    "                if present_conds==0:\n",
    "                    remove_conds.append(cond)\n",
    "            \n",
    "            condition_names=[x for x in condition_names if x not in remove_conds]\n",
    "            \n",
    "            onsets_df = map_spec_to_model(events_df, model_def)\n",
    "        \n",
    "           #editing filenames to allow for multiple tasks     \n",
    "            realign_file = os.path.join(PROJECT_DIR, 'working','nipype',\n",
    "                                        'workingdir_model_{}_{}_{}'.format(TASK_NAME[0].upper(),TASK_NAME[1].upper(),MODEL_NAME),\n",
    "                                        '{}-task-{}-run-0{}-realign.txt'.format(subject_id, task, run_num))\n",
    "\n",
    "            confound_file=os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                   '{}_task-{}_run-0{}_desc-confounds-regressors.tsv'.format(subject_id, \n",
    "                                                                                             task,\n",
    "                                                                                             run_num)\n",
    "                                   )\n",
    "\n",
    "            confound_df = pd.read_csv(confound_file, sep='\\t')\n",
    "\n",
    "            cols_to_use = [ 'TransX','TransY', 'TransZ', 'RotX', 'RotY', 'RotZ']\n",
    "\n",
    "            confound_df[cols_to_use].to_csv(realign_file, \n",
    "                                        header=False, \n",
    "                                        index=False,\n",
    "                                        sep='\\t')\n",
    "\n",
    "            realign_files.append(realign_file)\n",
    "\n",
    "            onsets = []\n",
    "            dur = []\n",
    "            #cscholz hack to allow for missing conditions\n",
    "            for cond in condition_names:\n",
    "            #for cond in model_def['Conditions']:\n",
    "                onsets.append(onsets_df[onsets_df.trial_type==cond].onset.values)\n",
    "                dur.append(onsets_df[onsets_df.trial_type==cond].duration.values)\n",
    "\n",
    "            \n",
    "         \n",
    "            #pmod = make_pmod(rdf, condition_names)   \n",
    "        \n",
    "            if model_def.get('Modulators'):\n",
    "                pmod = make_pmod(onsets_df, condition_names, \n",
    "                                pmods=model_def['Modulators'])     \n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "            subject_info.append(Bunch(conditions=condition_names,\n",
    "                         onsets=onsets,\n",
    "                         durations=dur,\n",
    "                         amplitudes=None,\n",
    "                         tmod=None,\n",
    "                         pmod=pmod,\n",
    "                         regressor_names=None,\n",
    "                         regressors=None))\n",
    "    \n",
    "    #cscholz: hack to allow for missing conditions\n",
    "    condition_names=model_def['Conditions'].keys()\n",
    "    \n",
    "    DM_regressors = []\n",
    "    for cond in condition_names:\n",
    "        DM_regressors.append(cond)\n",
    "        if pmod and model_def['Modulators'].get(cond):\n",
    "            DM_regressors.append('{}x{}^1'.format(cond, model_def['Modulators'].get(cond)))\n",
    "    \n",
    "    \n",
    "    return subject_info, realign_files, DM_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Bunch(amplitudes=None,\n",
       "        conditions=['message'],\n",
       "        durations=[array([ 14.610247,  14.612541,  14.229069,  15.062439,  20.728531,\n",
       "                 16.729065,  13.94577 ,  16.062469,  15.895508,  16.545639,\n",
       "                 17.545517,  21.01178 ,  14.395844,  30.010376,  19.512085,\n",
       "                 13.895721,  15.0625  ,  20.678406,  23.327942,  18.545319,\n",
       "                 14.328979,  18.94519 ,  20.44519 ])],\n",
       "        onsets=[array([   0.      ,   22.411564,   44.25906 ,   66.189499,   90.51973 ,\n",
       "                 118.7995  ,  143.163452,  164.310532,  190.024353,  213.037949,\n",
       "                 237.968185,  265.698029,  296.427643,  318.108521,  355.286804,\n",
       "                 383.033783,  405.264099,  428.744568,  457.124542,  489.720367,\n",
       "                 517.0672  ,  538.481079,  565.510925])],\n",
       "        pmod=[Bunch(name=['population_CTR_rank'],\n",
       "               param=[[-6.0,\n",
       "                 -1.0,\n",
       "                 11.0,\n",
       "                 -11.0,\n",
       "                 6.0,\n",
       "                 3.0,\n",
       "                 -10.0,\n",
       "                 4.0,\n",
       "                 -4.0,\n",
       "                 0.0,\n",
       "                 -2.0,\n",
       "                 5.0,\n",
       "                 -9.0,\n",
       "                 -7.0,\n",
       "                 9.0,\n",
       "                 1.0,\n",
       "                 7.0,\n",
       "                 10.0,\n",
       "                 -5.0,\n",
       "                 -3.0,\n",
       "                 2.0,\n",
       "                 -8.0,\n",
       "                 8.0]],\n",
       "               poly=[1])],\n",
       "        regressor_names=None,\n",
       "        regressors=None,\n",
       "        tmod=None),\n",
       "  Bunch(amplitudes=None,\n",
       "        conditions=['message', 'other'],\n",
       "        durations=[array([ 4.003273,  4.00354 ,  4.004776,  4.004463,  4.003624,  4.004517,\n",
       "                 4.003342,  4.004044,  3.988754,  4.005432,  3.990021,  4.003815,\n",
       "                 4.003571,  4.004913,  3.991028,  3.989502,  4.004456,  3.989258,\n",
       "                 4.004883,  4.005127]),\n",
       "         array([ 3.985606,  3.      ,  3.992725,  3.      ,  3.99313 ,  3.      ,\n",
       "                 4.004799,  3.      ,  3.993248,  3.      ,  3.      ,  3.      ,\n",
       "                 4.005203,  3.      ,  3.      ,  3.      ,  3.      ,  4.00531 ,\n",
       "                 3.      ,  3.993271,  3.      ,  4.005402,  3.      ,  3.993042,\n",
       "                 3.      ,  3.      ,  3.      ,  3.      ,  3.993042,  3.      ,\n",
       "                 3.      ,  4.004532,  3.      ,  4.005539,  3.      ,  3.      ,\n",
       "                 3.      ,  4.005371,  3.      ,  3.      ,  3.993073,  3.      ,\n",
       "                 3.993378,  3.      ,  3.993103,  3.      ,  4.005463,  3.      ,\n",
       "                 4.004974,  3.      ,  3.      ,  3.      ,  3.      ,  3.      ,\n",
       "                 3.      ,  3.      ,  4.005127,  3.      ,  3.      ,  3.      ])],\n",
       "        onsets=[array([  54.558891,   65.207283,   88.086578,   98.218941,  109.618385,\n",
       "                 167.243668,  178.593445,  190.024673,  213.003769,  245.999664,\n",
       "                 260.429962,  282.646759,  345.272583,  356.519836,  367.249054,\n",
       "                 379.615692,  390.466156,  400.813354,  423.511566,  433.910065]),\n",
       "         array([   0.      ,    3.985606,   10.791504,   14.784229,   21.023167,\n",
       "                  25.016296,   31.726898,   35.731697,   44.003559,   47.996807,\n",
       "                  58.562164,   69.210823,   75.421021,   79.426224,   92.091354,\n",
       "                 102.223404,  113.622009,  122.815063,  126.820374,  135.542206,\n",
       "                 139.535477,  145.828781,  149.834183,  156.006561,  159.999603,\n",
       "                 171.248184,  182.596786,  194.028717,  200.734329,  204.727371,\n",
       "                 216.992523,  224.803207,  228.807739,  234.90094 ,  238.906479,\n",
       "                 250.005096,  264.419983,  271.979858,  275.985229,  286.650574,\n",
       "                 293.139526,  297.132599,  303.487946,  307.481323,  313.736938,\n",
       "                 317.730042,  324.05661 ,  328.062073,  334.95578 ,  338.960754,\n",
       "                 349.276154,  360.52475 ,  371.240082,  383.605194,  394.470612,\n",
       "                 404.802612,  413.345947,  417.351074,  427.516449,  437.915192])],\n",
       "        pmod=[Bunch(name=['population_CTR_rank'],\n",
       "               param=[[-12.2,\n",
       "                 -15.2,\n",
       "                 -8.2,\n",
       "                 15.8,\n",
       "                 12.8,\n",
       "                 6.800000000000001,\n",
       "                 5.800000000000001,\n",
       "                 -10.2,\n",
       "                 1.8000000000000007,\n",
       "                 -0.1999999999999993,\n",
       "                 -7.199999999999999,\n",
       "                 -3.1999999999999993,\n",
       "                 -14.2,\n",
       "                 -9.2,\n",
       "                 20.8,\n",
       "                 -6.199999999999999,\n",
       "                 11.8,\n",
       "                 -11.2,\n",
       "                 7.800000000000001,\n",
       "                 13.8]],\n",
       "               poly=[1]),\n",
       "         None],\n",
       "        regressor_names=None,\n",
       "        regressors=None,\n",
       "        tmod=None),\n",
       "  Bunch(amplitudes=None,\n",
       "        conditions=['message', 'other'],\n",
       "        durations=[array([ 4.005245,  4.004673,  4.00528 ,  4.004105,  4.004875,  4.004929,\n",
       "                 4.004456,  3.989471,  3.989075,  4.00412 ,  4.00473 ,  3.99054 ,\n",
       "                 4.004974,  4.005463,  4.005127,  4.004517,  4.004395,  4.004395,\n",
       "                 4.0047  ,  3.989716]),\n",
       "         array([ 4.005342,  3.      ,  3.      ,  3.993103,  3.      ,  3.      ,\n",
       "                 4.004391,  3.      ,  3.993187,  3.      ,  3.993309,  3.      ,\n",
       "                 3.      ,  3.      ,  4.005165,  3.      ,  3.      ,  4.004868,\n",
       "                 3.      ,  3.989227,  3.      ,  4.004395,  3.      ,  3.      ,\n",
       "                 3.993195,  3.      ,  3.      ,  4.005356,  3.      ,  3.      ,\n",
       "                 3.993271,  3.      ,  3.      ,  3.989471,  3.      ,  3.      ,\n",
       "                 3.      ,  3.      ,  3.      ,  4.005188,  3.      ,  3.993256,\n",
       "                 3.      ,  3.993164,  3.      ,  3.      ,  3.      ,  3.      ,\n",
       "                 3.      ,  3.993256,  3.      ,  4.004822,  3.      ,  3.      ,\n",
       "                 3.      ,  3.993195,  3.      ,  3.      ,  3.993011,  3.      ])],\n",
       "        onsets=[array([  10.815418,   31.763355,   75.440712,   88.106964,  109.636856,\n",
       "                 156.031052,  178.612076,  200.75766 ,  224.821732,  245.987335,\n",
       "                 260.418304,  271.981079,  282.631989,  324.059692,  334.958679,\n",
       "                 345.274658,  356.523407,  390.48587 ,  400.83429 ,  423.529785]),\n",
       "         array([   0.      ,    4.005342,   14.820663,   21.059616,   25.052719,\n",
       "                  35.768028,   44.045479,   48.04987 ,   54.605339,   58.598526,\n",
       "                  65.237282,   69.230591,   79.445992,   92.111069,   98.237976,\n",
       "                 102.243141,  113.641731,  122.83522 ,  126.840088,  135.565964,\n",
       "                 139.555191,  145.849533,  149.853928,  160.03598 ,  167.27475 ,\n",
       "                 171.267944,  182.616531,  190.043137,  194.048492,  204.747131,\n",
       "                 213.019012,  217.012283,  228.810806,  234.903397,  238.892868,\n",
       "                 249.991455,  264.423035,  275.971619,  286.636963,  293.130493,\n",
       "                 297.135681,  303.491119,  307.484375,  313.739929,  317.733093,\n",
       "                 328.065155,  338.963806,  349.279175,  360.527802,  367.249878,\n",
       "                 371.243134,  379.620117,  383.624939,  394.490265,  404.838989,\n",
       "                 413.377563,  417.370758,  427.519501,  433.908508,  437.90152 ])],\n",
       "        pmod=[Bunch(name=['population_CTR_rank'],\n",
       "               param=[[0.25,\n",
       "                 -3.75,\n",
       "                 -5.75,\n",
       "                 15.25,\n",
       "                 1.25,\n",
       "                 -6.75,\n",
       "                 -19.75,\n",
       "                 -20.75,\n",
       "                 14.25,\n",
       "                 -15.75,\n",
       "                 -7.75,\n",
       "                 -1.75,\n",
       "                 12.25,\n",
       "                 7.25,\n",
       "                 2.25,\n",
       "                 6.25,\n",
       "                 17.25,\n",
       "                 16.25,\n",
       "                 -18.75,\n",
       "                 8.25]],\n",
       "               poly=[1]),\n",
       "         None],\n",
       "        regressor_names=None,\n",
       "        regressors=None,\n",
       "        tmod=None)],\n",
       " ['/data00/projects/megameta/project1/working/nipype/workingdir_model_BANNER_IMAGE_pmod_pop_rank/sub-P100-task-banner-run-01-realign.txt',\n",
       "  '/data00/projects/megameta/project1/working/nipype/workingdir_model_BANNER_IMAGE_pmod_pop_rank/sub-P100-task-image-run-01-realign.txt',\n",
       "  '/data00/projects/megameta/project1/working/nipype/workingdir_model_BANNER_IMAGE_pmod_pop_rank/sub-P100-task-image-run-02-realign.txt'],\n",
       " ['message', 'messagexpopulation_CTR_rank^1', 'other'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_info('sub-P100',os.path.join('/data00/projects/megameta/scripts/jupyter_megameta/first_level_models',\n",
    "                               PROJECT_NAME, 'model_specifications',\n",
    "                               MODEL_SPEC_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up contrasts\n",
    "\n",
    "* This part of the template needs work to provide a cleaner way to specify contrasts\n",
    "* Could use the same vector contrasts approach as we have in batch8 and then have a function to convert this into the list of list data structure that nipype spm contrasts node looks for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_contrast_list(subject_id, condition_names, model_path, DEBUG=False):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    condition_names.append('constant')\n",
    "\n",
    "    cont = []\n",
    "    for idx, cname in enumerate(condition_names):\n",
    "        ccode = [0 if pos!=idx else 1 for pos in range(len(condition_names))]\n",
    "        cont.append([cname, 'T', condition_names, ccode])\n",
    "\n",
    "    # add custom contrasts from the JSON model file\n",
    "    with open(model_path) as fh:\n",
    "        model_def = json.load(fh)\n",
    "    \n",
    "    contrasts = model_def.get('Contrasts')\n",
    "    \n",
    "    if not contrasts:\n",
    "        return cont\n",
    "    \n",
    "    for contrast in contrasts:\n",
    "            \n",
    "        cname = contrast['name']\n",
    "        \n",
    "        \n",
    "        \n",
    "        pos_idx = [condition_names.index(p) for p in contrast['pos']]\n",
    "        neg_idx = [condition_names.index(n) for n in contrast['neg']]\n",
    "        \n",
    "        pos_length = len(contrast['pos'])\n",
    "        neg_length = len(contrast['neg'])\n",
    "        \n",
    "        ccode = []\n",
    "        for idx, _ in enumerate(condition_names):\n",
    "            if idx in pos_idx:\n",
    "                ccode.append(1/pos_length)\n",
    "            elif idx in neg_idx:\n",
    "                ccode.append(-1/neg_length)\n",
    "            else:\n",
    "                ccode.append(0)\n",
    "\n",
    "        cont.append([cname, 'T', condition_names, ccode])\n",
    "        \n",
    "                \n",
    "        if DEBUG:\n",
    "            print(contrast)\n",
    "            print(ccode)\n",
    "            \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up processing nodes for modeling workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify model node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SpecifyModel - Generates SPM-specific Model\n",
    "modelspec = pe.Node(model.SpecifySPMModel(concatenate_runs=False,\n",
    "                                 input_units='secs',\n",
    "                                 output_units='secs',\n",
    "                                 time_repetition=TR,\n",
    "                                 high_pass_filter_cutoff=128),\n",
    "                                 output_units = 'scans',\n",
    "                 name=\"modelspec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 1 Design node\n",
    "\n",
    "** TODO -- get the right matching template file for fmriprep **\n",
    "\n",
    "* ??do we need a different mask than:\n",
    "\n",
    "    `'/data00/tools/spm8/apriori/brainmask_th25.nii'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Level1Design - Generates an SPM design matrix\n",
    "level1design = pe.Node(spm.Level1Design(bases={'hrf': {'derivs': [0, 0]}},\n",
    "                                 timing_units='secs',\n",
    "                                 interscan_interval=TR,\n",
    "                                 model_serial_correlations='none', #'AR(1)',\n",
    "                                 mask_image = '/data00/tools/spm8/apriori/brainmask_th25.nii',\n",
    "                                 global_intensity_normalization='none'\n",
    "                                       ),\n",
    "                    name=\"level1design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Model node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EstimateModel - estimate the parameters of the model\n",
    "level1estimate = pe.Node(spm.EstimateModel(estimation_method={'Classical': 1}),\n",
    "                      name=\"level1estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Contrasts node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EstimateContrast - estimates contrasts\n",
    "conestimate = pe.Node(spm.EstimateContrast(), name=\"conestimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pipeline workflow for level 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = pe.Workflow(name='l1analysis')\n",
    "\n",
    "# Connect up the 1st-level analysis components\n",
    "l1analysis.connect([(modelspec, level1design, [('session_info',\n",
    "                                                'session_info')]),\n",
    "                    (level1design, level1estimate, [('spm_mat_file',\n",
    "                                                     'spm_mat_file')]),\n",
    "                    (level1estimate, conestimate, [('spm_mat_file',\n",
    "                                                    'spm_mat_file'),\n",
    "                                                   ('beta_images',\n",
    "                                                    'beta_images'),\n",
    "                                                   ('residual_image',\n",
    "                                                    'residual_image')])\n",
    "                   \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up nodes for file handling and subject selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `getsubjectinfo` node \n",
    "\n",
    "* Use `get_subject_info()` function to generate spec data structure for first level model design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Subject Info - get subject specific condition information\n",
    "getsubjectinfo = pe.Node(util.Function(input_names=['subject_id', 'model_path'],\n",
    "                               output_names=['subject_info', 'realign_params', 'condition_names'],\n",
    "                               function=get_subject_info),\n",
    "                      name='getsubjectinfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makecontrasts = pe.Node(util.Function(input_names=['subject_id', 'condition_names', 'model_path'],\n",
    "                                     output_names=['contrasts'],\n",
    "                                      function=make_contrast_list),\n",
    "                    name='makecontrasts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `infosource` node\n",
    "\n",
    "* iterate over list of subject ids and generate subject ids and produce list of contrasts for subsequent nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = pe.Node(util.IdentityInterface(fields=['subject_id', 'model_path', 'resolution', 'smoothing']\n",
    "                                   ),\n",
    "                  name=\"infosource\")\n",
    "\n",
    "try:\n",
    "    fwhm_list = smoothing_list\n",
    "except:\n",
    "    fwhm_list = [4,6,8]\n",
    "    \n",
    "try:\n",
    "    resolution_list = resolutions\n",
    "except:\n",
    "    resolution_list = ['low','medium','high']\n",
    "\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('model_path', [JSON_MODEL_FILE]*len(subject_list)),\n",
    "                        ('resolution', resolution_list),\n",
    "                        ('smoothing', ['fwhm_{}'.format(s) for s in fwhm_list])\n",
    "                       ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selectfiles` node\n",
    "\n",
    "* match template to find source files (functional) for use in subsequent parts of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SelectFiles - to grab the data (alternativ to DataGrabber)\n",
    "\n",
    "## TODO: here need to figure out how to incorporate the run number and task name in call\n",
    "templates = {'func': '{subject_id}/{resolution}/{smoothing}/sr{subject_id}_task-'+TASK_PATTERN+'_run-0*_space-MNI152-T1-1mm_desc-preproc_bold.nii'}          \n",
    "\n",
    "selectfiles = pe.Node(nio.SelectFiles(templates,\n",
    "                               base_directory='/data00/projects/megameta/{}/derivatives/nipype/resampled_and_smoothed'.format(PROJECT_NAME)),\n",
    "                      working_dir=working_dir,\n",
    "                   name=\"selectfiles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify datasink node\n",
    "\n",
    "* copy files to keep from various working folders to output folder for model for subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datasink - creates output folder for important outputs\n",
    "datasink = pe.Node(nio.DataSink(base_directory=SUBJ_DIR,\n",
    "                         parameterization=True, \n",
    "                         #container=output_dir      \n",
    "                               ),\n",
    "                name=\"datasink\")\n",
    "\n",
    "datasink.inputs.base_directory = output_dir\n",
    "\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = []\n",
    "subjFolders = [('_model_path.*resolution_(low|medium|high)_smoothing_(fwhm_\\\\d{1,2})_subject_id_sub-.*/(.*)$', '\\\\1/\\\\2/\\\\3')]\n",
    "substitutions.extend(subjFolders)\n",
    "datasink.inputs.regexp_substitutions = substitutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workflow for whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = pe.Workflow(name='first_level_model_{}_{}_{}'.format(TASK_NAME[0].upper(),\n",
    "                                                                TASK_NAME[1].upper(),\n",
    "                                                                MODEL_NAME))\n",
    "pipeline.base_dir = os.path.join(SUBJ_DIR, working_dir)\n",
    "\n",
    "\n",
    "pipeline.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                             ('resolution', 'resolution'),\n",
    "                                             ('smoothing', 'smoothing')\n",
    "                                            ]),\n",
    "                  (infosource, getsubjectinfo, [('subject_id', 'subject_id'),\n",
    "                                                ('model_path', 'model_path')\n",
    "                                               ]),\n",
    "                  (infosource, makecontrasts, [('subject_id', 'subject_id'),\n",
    "                                               ('model_path', 'model_path')\n",
    "                                              ]),\n",
    "                  (getsubjectinfo, makecontrasts, [('condition_names', 'condition_names')]),\n",
    "                  \n",
    "                 (getsubjectinfo, l1analysis, [('subject_info',\n",
    "                                                 'modelspec.subject_info'),\n",
    "                                                ('realign_params',\n",
    "                                                   'modelspec.realignment_parameters')]),\n",
    "                  (makecontrasts, l1analysis, [('contrasts',\n",
    "                                             'conestimate.contrasts')]),\n",
    "                  \n",
    "                  \n",
    "                  (selectfiles, l1analysis, [('func',\n",
    "                                          'modelspec.functional_runs')]),\n",
    "                  \n",
    "                                    \n",
    "                  (infosource, datasink, [('subject_id','container')]),\n",
    "                  (l1analysis, datasink, [('conestimate.spm_mat_file','@spm'),\n",
    "                                          ('level1estimate.beta_images','@betas'),\n",
    "                                          ('level1estimate.mask_image','@mask'),\n",
    "                                          ('conestimate.spmT_images','@spmT'),\n",
    "                                          ('conestimate.con_images','@con'),\n",
    "                                          ('conestimate.spmF_images','@spmF')\n",
    "                                         ])\n",
    "                 ]\n",
    ")\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
