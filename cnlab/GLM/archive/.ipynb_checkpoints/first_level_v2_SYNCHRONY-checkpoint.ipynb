{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building first level models using _nipype_ and _SPM12_\n",
    "\n",
    "## Base functionality for _megameta_ project\n",
    "\n",
    "-------\n",
    "#### History\n",
    "\n",
    "*4/10/19 cscholz - updated template file names for Synchrony\n",
    "* 4/9/19 cscholz - made small correction to make_contrast_list() (setting: -1/neg_length instead of -1/pos_length)\n",
    "* 4/2/19 mbod - split out processing pipeline for revised workflow\n",
    "* 3/28/19 mbod - update pipeline to include resampling to template & SPM path reference\n",
    "* 3/23/19 mbod - include contrast definition in the config JSON file\n",
    "* 3/9/19 mbod - updates from testing template with `darpa1`\n",
    "* 2/27/19 mbod  - modify example notebook to make base functionality notebook\n",
    "\n",
    "-----\n",
    "\n",
    "### Description\n",
    "\n",
    "* Set up a nipype workflow to use SPM12 to make first level models for _megameta_ task data (preprocessed using `batch8` SPM8 scripts) in BIDS derivative format   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### Template variables\n",
    "\n",
    "* Specify the following values:\n",
    "    1. project name - should be name of folder under `/data00/project/megameta`, e.g. `project1`\n",
    "    2. filename for JSON model specification (should be inside `model_specification` folder), e.g. `p1_image_pmod_likeme.json`\n",
    "    3. TR value in seconds\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### Setup\n",
    "\n",
    "* import required modules and define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # system functions\n",
    "\n",
    "# NIYPE FUNCTIONS\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.interfaces.spm as spm          # spm\n",
    "import nipype.interfaces.matlab as mlab      # how to run matlab\n",
    "import nipype.interfaces.utility as util     # utility\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from nilearn import plotting, image\n",
    "from nistats import thresholding\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SPEC_FILE = 'synchrony_model_comm_pmod_pop_rank.json' # replace with filename of JSON file\n",
    "PROJECT_NAME = 'synchrony' # replace with project name (matching folder name in megameta, e.g. PA2, darpa1)\n",
    "TR=1  # specify task TR\n",
    "PATH_TO_SPM_FOLDER = '/data00/tools/spm12mega'\n",
    "\n",
    "resolutions = ['medium']\n",
    "smoothing_list = [8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matlab path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the way matlab should be called\n",
    "mlab.MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "# If SPM is not in your MATLAB path you should add it here\n",
    "mlab.MatlabCommand.set_default_paths(PATH_TO_SPM_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* These need to be reformatted to be consistent\n",
    "* as data is not smoothed commented out the `fwhm_size` param - but data probably has a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_MODEL_FILE = os.path.join('/data00/projects/megameta/scripts/jupyter_megameta/first_level_models',\n",
    "                               PROJECT_NAME, 'model_specifications',\n",
    "                               MODEL_SPEC_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(JSON_MODEL_FILE) as fh:\n",
    "    model_def = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASK_NAME = model_def['TaskName']\n",
    "RUNS = model_def['Runs']\n",
    "MODEL_NAME = model_def['ModelName']\n",
    "PROJECT_NAME = model_def['ProjectID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join('/data00/projects/megameta', PROJECT_NAME)\n",
    "SUBJ_DIR = os.path.join(PROJECT_DIR, 'derivatives', 'nipype', 'resampled_and_smoothed')\n",
    "\n",
    "\n",
    "task_func_template = \"sr{PID}_task-{TASK}_run-0{RUN}_bold_space-MNI152NLin2009cAsym_preproc.nii\"\n",
    "\n",
    "\n",
    "subject_list = [subj for subj in os.listdir(SUBJ_DIR) \n",
    "                   if os.path.exists(os.path.join(SUBJ_DIR,subj,'medium', 'fwhm_8',\n",
    "                                        task_func_template.format(PID=subj, TASK=TASK_NAME, RUN=1)))]\n",
    "\n",
    "output_dir = os.path.join(PROJECT_DIR,'derivatives', 'nipype','model_{}_{}'.format(TASK_NAME.upper(), MODEL_NAME))        # name of 1st-level output folder\n",
    "working_dir = os.path.join(PROJECT_DIR, 'working', \n",
    "                           'nipype', 'workingdir_model_{}_{}'.format(TASK_NAME.upper(), MODEL_NAME))   # name of 1st-level working directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check to see if output and work directories exist\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir) \n",
    "\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No subject exclusions applied\n",
      "\n",
      "\n",
      "No subject inclusions applied\n",
      "\n",
      "\n",
      "SUBJECT LIST IS:\n",
      "\t sub-SF09 sub-SF16 sub-SF04 sub-SF01 sub-SF15 sub-SF03 sub-SF13 sub-SF11 sub-SF02 sub-SF10 sub-SF08 sub-SF06 sub-SF05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    subject_list = [ s for s in subject_list if s not in exclude_subjects ]\n",
    "    print('\\n\\nApplied subject inclusion list:\\n\\t',' '.join(exclude_subjects))\n",
    "except:\n",
    "    print('\\n\\nNo subject exclusions applied')\n",
    "\n",
    "try:\n",
    "    subject_list = [ s for s in subject_list if s in include_subjects ]\n",
    "    print('\\n\\nApplied subject inclusion list:\\n\\t',' '.join(include_subjects))\n",
    "except:\n",
    "    print('\\n\\nNo subject inclusions applied')\n",
    "\n",
    "    \n",
    "print('\\n\\nSUBJECT LIST IS:\\n\\t', ' '.join(subject_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for subject info and contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup design matrix data for subject\n",
    "\n",
    "* need a function to set up the nipype `Bunch` format used\n",
    "    * https://nipype.readthedocs.io/en/latest/users/model_specification.html\n",
    "* read the onsets/dur/conditions from task logs and extract needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject_info(subject_id, model_path, DEBUG=False):\n",
    "    '''\n",
    "    1. load model specification from JSON spec file\n",
    "    2. get confound file for subject for task to add to design matrix     \n",
    "    3. get task spec CSV for subject for task\n",
    "    4. setup subject info structure\n",
    "    '''\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    from nipype.interfaces.base import Bunch\n",
    "    \n",
    "    \n",
    "    def make_pmod(df, conditions, pmods={}, normalize='mean'):\n",
    "        \n",
    "        pmod = []\n",
    "        \n",
    "        for cond in conditions:\n",
    "        \n",
    "            \n",
    "            if not pmods.get(cond):\n",
    "                pmod.append(None)\n",
    "            else:\n",
    "                df2 = df[df.trial_type==cond]\n",
    "                \n",
    "                pmod_name = pmods.get(cond)\n",
    "                \n",
    "                #pmod = [pmod] if not type(pmods) is list else pmod\n",
    "                \n",
    "                # MAKE SURE THERE IS VARIANCE IN PMOD VECTOR\n",
    "                if df2[pmod_name].var()==0:\n",
    "                    df2[pmod_name]+=0.001\n",
    "                    \n",
    "                # APPLY NORMALIZATION\n",
    "                if normalize=='mean':\n",
    "                    df2[pmod_name] = df2[pmod_name] - df2[pmod_name].mean()\n",
    "\n",
    "                pmod.append(Bunch(name=[pmod_name],\n",
    "                      param=[df2[pmod_name].values.tolist()\n",
    "                            ],\n",
    "                      poly=[1]\n",
    "                     ))\n",
    "        \n",
    "        return pmod\n",
    "    \n",
    "    \n",
    "\n",
    "    def map_spec_to_model(spec_df,model):\n",
    "        \"\"\"\n",
    "        Maps spec trial names to model contrast trials.\n",
    "\n",
    "        Args:\n",
    "            spec: the events.tsv spec file\n",
    "            model: the model.json file\n",
    "\n",
    "        Returns:\n",
    "            pandas dataframe object\n",
    "        \"\"\"\n",
    "\n",
    "        spec=spec_df.copy()\n",
    "\n",
    "        for con in model['Conditions']:\n",
    "            spec_trials = model['Conditions'][con]\n",
    "            spec.loc[spec.trial_type.isin(spec_trials),'trial_type'] = con\n",
    "            spec.onset.sort_values()\n",
    "\n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    with open(model_path) as fh:\n",
    "        model_def = json.load(fh)\n",
    "    \n",
    "    \n",
    "    pmod = None if not model_def.get('Modulators') else []\n",
    "    \n",
    "    TASK_NAME = model_def['TaskName']\n",
    "    TASK_RUNS = model_def['Runs']\n",
    "    MODEL_NAME = model_def['ModelName']\n",
    "    PROJECT_ID = model_def['ProjectID']\n",
    "    \n",
    "    condition_names = list(model_def['Conditions'].keys())\n",
    "   \n",
    "    \n",
    "    PROJECT_DIR = os.path.join('/data00/projects/megameta', PROJECT_ID)\n",
    "    SUBJ_DIR = os.path.join(PROJECT_DIR,'derivatives', 'fmriprep')\n",
    "    \n",
    "    realign_files = []\n",
    "    subject_info = []\n",
    "    \n",
    "\n",
    "    \n",
    "    # check to see which runs exist for subject\n",
    "    # by looking for appropriate events.tsv files\n",
    "    # this could (should?) also include looking for the nifti file?\n",
    "    runs_for_subj = [run for run in TASK_RUNS\n",
    "                    if \n",
    "                    os.path.exists(os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                    '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                           TASK_NAME,\n",
    "                                                                           run)))\n",
    "                    ]\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"runs_for_subj\", runs_for_subj)\n",
    "        print(\"checked paths:\")\n",
    "        for run in TASK_RUNS:\n",
    "            print('\\t', os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                    '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                           TASK_NAME,\n",
    "                                                                           run)))\n",
    "        print(\"TASK NAME\", TASK_NAME)\n",
    "        print(\"pmod\", pmod)\n",
    "        print(\"TASK_RUNS\", TASK_RUNS)\n",
    "        print(\"subject_id\", subject_id)\n",
    "    \n",
    "    \n",
    "    for run_num, _ in enumerate(runs_for_subj,1):\n",
    "    \n",
    "    \n",
    "        events_df = pd.read_csv(os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                             '{}_task-{}_run-0{}_events.tsv'.format(subject_id, \n",
    "                                                                                    TASK_NAME,\n",
    "                                                                                    run_num)),\n",
    "                               sep='\\t')\n",
    "\n",
    "\n",
    "        onsets_df = map_spec_to_model(events_df, model_def)\n",
    "        \n",
    "                \n",
    "        realign_file = os.path.join(PROJECT_DIR, 'working','nipype',\n",
    "                                        'workingdir_model_{}_{}'.format(TASK_NAME.upper(),MODEL_NAME),\n",
    "                                        '{}-run-0{}-realign.txt'.format(subject_id, run_num))\n",
    "\n",
    "        confound_file=os.path.join(SUBJ_DIR, subject_id, 'func',\n",
    "                                   '{}_task-{}_run-0{}_bold_confounds.tsv'.format(subject_id, \n",
    "                                                                                             TASK_NAME,\n",
    "                                                                                             run_num)\n",
    "                                   )\n",
    "\n",
    "        confound_df = pd.read_csv(confound_file, sep='\\t')\n",
    "\n",
    "        cols_to_use = [ 'X','Y', 'Z', 'RotX', 'RotY', 'RotZ']\n",
    "\n",
    "        confound_df[cols_to_use].to_csv(realign_file, \n",
    "                                        header=False, \n",
    "                                        index=False,\n",
    "                                        sep='\\t')\n",
    "\n",
    "        realign_files.append(realign_file)\n",
    "\n",
    "        onsets = []\n",
    "        dur = []\n",
    "        for cond in model_def['Conditions']:\n",
    "            onsets.append(onsets_df[onsets_df.trial_type==cond].onset.values)\n",
    "            dur.append(onsets_df[onsets_df.trial_type==cond].duration.values)\n",
    "\n",
    "            \n",
    "         \n",
    "        #pmod = make_pmod(rdf, condition_names)   \n",
    "        \n",
    "        if model_def.get('Modulators'):\n",
    "            pmod = make_pmod(onsets_df, condition_names, \n",
    "                         pmods=model_def['Modulators'])     \n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        subject_info.append(Bunch(conditions=condition_names,\n",
    "                         onsets=onsets,\n",
    "                         durations=dur,\n",
    "                         amplitudes=None,\n",
    "                         tmod=None,\n",
    "                         pmod=pmod,\n",
    "                         regressor_names=None,\n",
    "                         regressors=None))\n",
    "    \n",
    "    \n",
    "    DM_regressors = []\n",
    "    for cond in condition_names:\n",
    "        DM_regressors.append(cond)\n",
    "        if pmod and model_def['Modulators'].get(cond):\n",
    "            DM_regressors.append('{}x{}^1'.format(cond, model_def['Modulators'].get(cond)))\n",
    "    \n",
    "    \n",
    "    return subject_info, realign_files, DM_regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up contrasts\n",
    "\n",
    "* This part of the template needs work to provide a cleaner way to specify contrasts\n",
    "* Could use the same vector contrasts approach as we have in batch8 and then have a function to convert this into the list of list data structure that nipype spm contrasts node looks for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_contrast_list(subject_id, condition_names, model_path, DEBUG=False):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    condition_names.append('constant')\n",
    "\n",
    "    cont = []\n",
    "    for idx, cname in enumerate(condition_names):\n",
    "        ccode = [0 if pos!=idx else 1 for pos in range(len(condition_names))]\n",
    "        cont.append([cname, 'T', condition_names, ccode])\n",
    "\n",
    "    # add custom contrasts from the JSON model file\n",
    "    with open(model_path) as fh:\n",
    "        model_def = json.load(fh)\n",
    "    \n",
    "    contrasts = model_def.get('Contrasts')\n",
    "    \n",
    "    if not contrasts:\n",
    "        return cont\n",
    "    \n",
    "    for contrast in contrasts:\n",
    "            \n",
    "        cname = contrast['name']\n",
    "        \n",
    "        \n",
    "        \n",
    "        pos_idx = [condition_names.index(p) for p in contrast['pos']]\n",
    "        neg_idx = [condition_names.index(n) for n in contrast['neg']]\n",
    "        \n",
    "        pos_length = len(contrast['pos'])\n",
    "        neg_length = len(contrast['neg'])\n",
    "        \n",
    "        ccode = []\n",
    "        for idx, _ in enumerate(condition_names):\n",
    "            if idx in pos_idx:\n",
    "                ccode.append(1/pos_length)\n",
    "            elif idx in neg_idx:\n",
    "                ccode.append(-1/neg_length)\n",
    "            else:\n",
    "                ccode.append(0)\n",
    "\n",
    "        cont.append([cname, 'T', condition_names, ccode])\n",
    "        \n",
    "                \n",
    "        if DEBUG:\n",
    "            print(contrast)\n",
    "            print(ccode)\n",
    "            \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up processing nodes for modeling workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify model node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SpecifyModel - Generates SPM-specific Model\n",
    "modelspec = pe.Node(model.SpecifySPMModel(concatenate_runs=False,\n",
    "                                 input_units='secs',\n",
    "                                 output_units='secs',\n",
    "                                 time_repetition=TR,\n",
    "                                 high_pass_filter_cutoff=128),\n",
    "                                 output_units = 'scans',\n",
    "                 name=\"modelspec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 1 Design node\n",
    "\n",
    "** TODO -- get the right matching template file for fmriprep **\n",
    "\n",
    "* ??do we need a different mask than:\n",
    "\n",
    "    `'/data00/tools/spm8/apriori/brainmask_th25.nii'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Level1Design - Generates an SPM design matrix\n",
    "level1design = pe.Node(spm.Level1Design(bases={'hrf': {'derivs': [0, 0]}},\n",
    "                                 timing_units='secs',\n",
    "                                 interscan_interval=TR,\n",
    "                                 model_serial_correlations='none', #'AR(1)',\n",
    "                                 mask_image = '/data00/tools/spm8/apriori/brainmask_th25.nii',\n",
    "                                 global_intensity_normalization='none'\n",
    "                                       ),\n",
    "                    name=\"level1design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Model node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EstimateModel - estimate the parameters of the model\n",
    "level1estimate = pe.Node(spm.EstimateModel(estimation_method={'Classical': 1}),\n",
    "                      name=\"level1estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Contrasts node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EstimateContrast - estimates contrasts\n",
    "conestimate = pe.Node(spm.EstimateContrast(), name=\"conestimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pipeline workflow for level 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = pe.Workflow(name='l1analysis')\n",
    "\n",
    "# Connect up the 1st-level analysis components\n",
    "l1analysis.connect([(modelspec, level1design, [('session_info',\n",
    "                                                'session_info')]),\n",
    "                    (level1design, level1estimate, [('spm_mat_file',\n",
    "                                                     'spm_mat_file')]),\n",
    "                    (level1estimate, conestimate, [('spm_mat_file',\n",
    "                                                    'spm_mat_file'),\n",
    "                                                   ('beta_images',\n",
    "                                                    'beta_images'),\n",
    "                                                   ('residual_image',\n",
    "                                                    'residual_image')])\n",
    "                   \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up nodes for file handling and subject selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `getsubjectinfo` node \n",
    "\n",
    "* Use `get_subject_info()` function to generate spec data structure for first level model design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Subject Info - get subject specific condition information\n",
    "getsubjectinfo = pe.Node(util.Function(input_names=['subject_id', 'model_path'],\n",
    "                               output_names=['subject_info', 'realign_params', 'condition_names'],\n",
    "                               function=get_subject_info),\n",
    "                      name='getsubjectinfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makecontrasts = pe.Node(util.Function(input_names=['subject_id', 'condition_names', 'model_path'],\n",
    "                                     output_names=['contrasts'],\n",
    "                                      function=make_contrast_list),\n",
    "                    name='makecontrasts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `infosource` node\n",
    "\n",
    "* iterate over list of subject ids and generate subject ids and produce list of contrasts for subsequent nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = pe.Node(util.IdentityInterface(fields=['subject_id', 'model_path', 'resolution', 'smoothing']\n",
    "                                   ),\n",
    "                  name=\"infosource\")\n",
    "\n",
    "try:\n",
    "    fwhm_list = smoothing_list\n",
    "except:\n",
    "    fwhm_list = [4,6,8]\n",
    "    \n",
    "try:\n",
    "    resolution_list = resolutions\n",
    "except:\n",
    "    resolution_list = ['low','medium','high']\n",
    "\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('model_path', [JSON_MODEL_FILE]*len(subject_list)),\n",
    "                        ('resolution', resolution_list),\n",
    "                        ('smoothing', ['fwhm_{}'.format(s) for s in fwhm_list])\n",
    "                       ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selectfiles` node\n",
    "\n",
    "* match template to find source files (functional) for use in subsequent parts of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SelectFiles - to grab the data (alternativ to DataGrabber)\n",
    "\n",
    "\n",
    "## TODO: here need to figure out how to incorporate the run number and task name in call\n",
    "templates = {'func': '{subject_id}/{resolution}/{smoothing}/sr{subject_id}_task-'+TASK_NAME+'_run-0*_bold_space-MNI152NLin2009cAsym_preproc.nii'}          \n",
    "\n",
    "selectfiles = pe.Node(nio.SelectFiles(templates,\n",
    "                               base_directory='/data00/projects/megameta/{}/derivatives/nipype/resampled_and_smoothed'.format(PROJECT_NAME)),\n",
    "                      working_dir=working_dir,\n",
    "                   name=\"selectfiles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify datasink node\n",
    "\n",
    "* copy files to keep from various working folders to output folder for model for subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datasink - creates output folder for important outputs\n",
    "datasink = pe.Node(nio.DataSink(base_directory=SUBJ_DIR,\n",
    "                         parameterization=True, \n",
    "                         #container=output_dir      \n",
    "                               ),\n",
    "                name=\"datasink\")\n",
    "\n",
    "datasink.inputs.base_directory = output_dir\n",
    "\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = []\n",
    "subjFolders = [('_model_path.*resolution_(low|medium|high)_smoothing_(fwhm_\\\\d{1,2})_subject_id_sub-.*/(.*)$', '\\\\1/\\\\2/\\\\3')]\n",
    "substitutions.extend(subjFolders)\n",
    "datasink.inputs.regexp_substitutions = substitutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workflow for whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = pe.Workflow(name='first_level_model_{}_{}'.format(TASK_NAME.upper(),MODEL_NAME))\n",
    "pipeline.base_dir = os.path.join(SUBJ_DIR, working_dir)\n",
    "\n",
    "\n",
    "pipeline.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                             ('resolution', 'resolution'),\n",
    "                                             ('smoothing', 'smoothing')\n",
    "                                            ]),\n",
    "                  (infosource, getsubjectinfo, [('subject_id', 'subject_id'),\n",
    "                                                ('model_path', 'model_path')\n",
    "                                               ]),\n",
    "                  (infosource, makecontrasts, [('subject_id', 'subject_id'),\n",
    "                                               ('model_path', 'model_path')\n",
    "                                              ]),\n",
    "                  (getsubjectinfo, makecontrasts, [('condition_names', 'condition_names')]),\n",
    "                  \n",
    "                 (getsubjectinfo, l1analysis, [('subject_info',\n",
    "                                                 'modelspec.subject_info'),\n",
    "                                                ('realign_params',\n",
    "                                                   'modelspec.realignment_parameters')]),\n",
    "                  (makecontrasts, l1analysis, [('contrasts',\n",
    "                                             'conestimate.contrasts')]),\n",
    "                  \n",
    "                  \n",
    "                  (selectfiles, l1analysis, [('func',\n",
    "                                          'modelspec.functional_runs')]),\n",
    "                  \n",
    "                                    \n",
    "                  (infosource, datasink, [('subject_id','container')]),\n",
    "                  (l1analysis, datasink, [('conestimate.spm_mat_file','@spm'),\n",
    "                                          ('level1estimate.beta_images','@betas'),\n",
    "                                          ('level1estimate.mask_image','@mask'),\n",
    "                                          ('conestimate.spmT_images','@spmT'),\n",
    "                                          ('conestimate.con_images','@con'),\n",
    "                                          ('conestimate.spmF_images','@spmF')\n",
    "                                         ])\n",
    "                 ]\n",
    ")\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
