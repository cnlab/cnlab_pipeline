{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Plug in the `model_path` (and change `script_path` and `singularity_path` if neccessary).\n",
    "2. From `Run` menu, select `Restart Kernel and Run All Cells`\n",
    "3. From `View` menu, select `Collapse All Code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data00/projects/megameta/scripts/jupyter_megameta/first_level_models/BA/model_specifications/task-walkstatement_model-messageUpdateTest.json'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = '/data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/first_level_HY.py'\n",
    "singularity_path = '/data00/tools/singularity_images/neurodocker.sqsh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, copy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_exist(path, file):\n",
    "    if os.path.exists(os.path.join(path, file)):\n",
    "        return file\n",
    "    else:\n",
    "        raise Exception(\"File missing: \" + file)\n",
    "\n",
    "def ensure_relative(path):\n",
    "    if path.startswith('/'):\n",
    "        raise Exception(\"Make sure path is relative to the data path: \" + path)\n",
    "    else:\n",
    "        return path\n",
    "        \n",
    "def ensure_list(obj):\n",
    "    if type(obj) is not list:\n",
    "        return [obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def remove_key(key, obj):\n",
    "    if obj.get(key):\n",
    "        del obj[key]\n",
    "        \n",
    "def copy_from_template(target, template):\n",
    "    for key, item in template.items():\n",
    "        if key not in target.keys():\n",
    "            target[key] = copy.deepcopy(template[key])\n",
    "        elif type(item) is dict:\n",
    "            copy_from_template(target[key], template[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template megameta_default.json: Mega-meta default pipeline\n",
      "\t6mm FWHM smoothing\n",
      "\tNo global scaling\n",
      "\tFAST correlation\n",
      "\t6 motion parameters and framewise displacement\n",
      "\tOutlier: FD > 0.75\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, 'r') as f:\n",
    "    model = json.load(f)\n",
    "    \n",
    "if model.get(\"Template\"):\n",
    "\n",
    "    template_path = model.get(\"Template\")\n",
    "    with open(template_path, 'r') as f:\n",
    "        template = json.load(f)\n",
    "    \n",
    "    print(f\"Using template {os.path.basename(template_path)}: \", end=\"\")\n",
    "    print(\"\\n\\t\".join(template.get(\"Descriptions\", [\"No description found\"])))\n",
    "\n",
    "    copy_from_template(model, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 35 participants:\n",
      "['BA224', 'BA227', 'BA228', 'BA233', 'BA229', 'BA234', 'BA235', 'BA237', 'BA238', 'BA239', 'BA240', 'BA242', 'BA244', 'BA247', 'BA250', 'BA251', 'BA252', 'BA253', 'BA255', 'BA257', 'BA258', 'BA261', 'BA262', 'BA263', 'BA265', 'BA266', 'BA267', 'BA269', 'BA272', 'BA273', 'BA275', 'BA276', 'BA277', 'BA278', 'BA245']\n"
     ]
    }
   ],
   "source": [
    "task = model['Info']['task']\n",
    "job_name = 'task-{}_model-{}'.format(model['Info']['task'], model['Info']['model'])\n",
    "\n",
    "env = model['Environment']\n",
    "\n",
    "os.makedirs(env['output_path'], exist_ok=True)\n",
    "os.makedirs(env['working_path'], exist_ok=True)\n",
    "\n",
    "try:\n",
    "    os.chmod(env['output_path'], 0o0777)\n",
    "    os.chmod(env['working_path'], 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "subs = []\n",
    "if model['Info'].get('sub'):\n",
    "    subs = ensure_list(model['Info']['sub'])\n",
    "elif model['Info'].get('sub_container'):\n",
    "    sub_container = ensure_relative(model['Info']['sub_container'])\n",
    "    subs = [x.split(os.path.sep)[-2].replace('sub-','') for x in glob.glob(os.path.join(env['data_path'], sub_container, 'sub-*'+os.path.sep))]\n",
    "\n",
    "exclude_subs = model['Info'].get('exclude',{}).get('sub',[])\n",
    "for es in exclude_subs:\n",
    "    if es in subs:\n",
    "        subs.remove(es)\n",
    "\n",
    "if len(subs) == 0:\n",
    "    raise Exception(\"No subjects found.\")\n",
    "else:\n",
    "    print(f\"Processing {len(subs)} participants:\")\n",
    "    print(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_event(env, job_name, spm_model, event_path):\n",
    "    \n",
    "    output_path = os.path.join(ensure_relative(env['job_path']), job_name, 'events') \n",
    "    event_df = pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "    \n",
    "    for operation, params in spm_model.get(\"event_options\", {}).items():\n",
    "        \n",
    "        if operation == 'map_condition':\n",
    "            \n",
    "            condition_map = {}\n",
    "            for trial_type in event_df['trial_type'].unique():\n",
    "                condition_map[trial_type] = trial_type\n",
    "                \n",
    "            for new_trial_type, trial_types in params.items():\n",
    "                for trial_type in ensure_list(trial_types):\n",
    "                    condition_map[trial_type] = new_trial_type\n",
    "            \n",
    "            event_df['trial_type'] = event_df['trial_type'].map(condition_map)\n",
    "            \n",
    "        elif operation == 'melt_condition':\n",
    "            \n",
    "            for trial_type, trial_data in params.items():\n",
    "                for idx, row in event_df.query(f'trial_type == \"{trial_type}\"').iterrows():\n",
    "                    event_df.loc[idx, 'trial_type'] = trial_type + \"_\" + str(row[trial_data])\n",
    "\n",
    "    all_pmods = []\n",
    "    for trial_type, pmods in spm_model.get('pmod',{}).items():\n",
    "        \n",
    "        trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "        for pmod in ensure_list(pmods):\n",
    "            all_pmods.append(pmod)\n",
    "            \n",
    "            pmod_values = trial_df[pmod].copy()\n",
    "            pmod_values = pmod_values.fillna(pmod_values.mean())\n",
    "            \n",
    "            for operation in ensure_list(spm_model.get(\"pmod_options\", [])):\n",
    "                \n",
    "                if operation == \"rank\":\n",
    "                    pmod_values = pmod_values.rank()\n",
    "                elif operation == \"minmax_scale\":\n",
    "                    pmod_values = (pmod_values - pmod_values.min()) / (pmod_values.max() - pmod_values.min())\n",
    "                elif operation == \"zscore\":\n",
    "                    pmod_values = (pmod_values - pmod_values.mean()) / pmod_values.std()\n",
    "            \n",
    "            event_df.loc[trial_df.index, pmod] = pmod_values.tolist()\n",
    "        \n",
    "    (event_df[['onset', 'duration', 'trial_type'] + np.unique(all_pmods).tolist()]\n",
    "         .sort_values(by='onset')\n",
    "         .to_csv(os.path.join(env['data_path'], output_path, os.path.basename(event_path)), sep='\\t', index=False))\n",
    "    \n",
    "    return os.path.join(output_path, os.path.basename(event_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA224 : "
     ]
    },
    {
     "ename": "Exception",
     "evalue": "File missing: derivatives/fmriprep/BA224/func/sub-BA224_task-walkstatement_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8151faa10ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mformat_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mfunctional_runs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspm_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'functional_runs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mformat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mregressors_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspm_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regressors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mformat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7103ddb44f6a>\u001b[0m in \u001b[0;36mensure_exist\u001b[0;34m(path, file)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File missing: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mensure_relative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: File missing: derivatives/fmriprep/BA224/func/sub-BA224_task-walkstatement_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz"
     ]
    }
   ],
   "source": [
    "spm_model = model['SpecifySPMModel']\n",
    "\n",
    "env['job_path'] = ensure_relative(env['job_path'])\n",
    "\n",
    "job_path = os.path.join(env['data_path'], env['job_path'], job_name, 'jobs') \n",
    "os.makedirs(job_path, exist_ok=True)\n",
    "\n",
    "if (spm_model.get(\"event_options\") is not None) or (spm_model.get(\"pmod_options\") is not None):\n",
    "    event_path = os.path.join(env['job_path'], job_name, 'events') \n",
    "    os.makedirs(os.path.join(env['data_path'], event_path), exist_ok=True)\n",
    "\n",
    "if spm_model.get(\"outlier\"):\n",
    "    outlier_path = os.path.join(env['job_path'], job_name, 'outlier') \n",
    "    os.makedirs(os.path.join(env['data_path'], outlier_path), exist_ok=True)\n",
    "    \n",
    "for regressors_path in ensure_list(spm_model.get('regressors', [])):\n",
    "    if (not regressors_path.endswith('txt')) and (not regressors_path.endswith('tsv')):\n",
    "        raise Exception(\"Regressors: only TSV or TXT file supported.\")\n",
    "\n",
    "regressor_names = ensure_list(spm_model.get('regressor_names',[[]]))\n",
    "if type(regressor_names[0]) is not list:\n",
    "    regressor_names = [regressor_names]\n",
    "    \n",
    "for sub in subs:\n",
    "    print(sub, \": \", end=\"\")\n",
    "    \n",
    "    job = copy.deepcopy(model)\n",
    "    \n",
    "    format_args = {}\n",
    "    format_args['sub'] = sub\n",
    "    format_args['task'] = task\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    runs = ensure_list(job['Info'].get('run',-1))\n",
    "    for exclude_sub, exclude_runs in job['Info'].get('exclude',{}).get('run',{}).items():\n",
    "        if sub == exclude_sub:\n",
    "            for exclude_run in ensure_list(exclude_runs):\n",
    "                if exclude_run in runs:\n",
    "                    runs.remove(exclude_run)\n",
    "                    \n",
    "    functional_runs = []\n",
    "    regressors = []\n",
    "    event_files = []\n",
    "    outlier_files = []\n",
    "\n",
    "    for run in runs:\n",
    "        \n",
    "        if run != -1:\n",
    "            format_args['run'] = run\n",
    "        \n",
    "        functional_runs.append(ensure_exist(env['data_path'], spm_model['functional_runs'].format(**format_args)))\n",
    "            \n",
    "        regressors_path = ensure_exist(env['data_path'], spm_model['regressors'].format(**format_args))\n",
    "        regressors.append(regressors_path)\n",
    "\n",
    "        if regressors_path.endswith('tsv'):\n",
    "            \n",
    "            regressor_df = pd.read_csv(os.path.join(env['data_path'], regressors_path), sep='\\t')\n",
    "\n",
    "            chosen_regressor_names = []\n",
    "            for rn in regressor_names:\n",
    "                col_exists = regressor_df.columns.isin(rn)\n",
    "                if col_exists.sum() > 0:\n",
    "                    chosen_regressor_names = regressor_df.columns[col_exists].tolist()\n",
    "                    if col_exists.sum() != len(rn):\n",
    "                        issues.append(f\"{regressors_path}: Only some of the regressors are found - {chosen_regressor_names}\")\n",
    "                    break\n",
    "\n",
    "            if len(chosen_regressor_names) == 0:\n",
    "                raise Exception(\"No regressors found.\")\n",
    "\n",
    "        if spm_model.get(\"outlier\"):\n",
    "            \n",
    "            outlier_indices = list(range(spm_model['outlier'].get(\"dummy_scan\", 0)))\n",
    "            \n",
    "            if spm_model[\"outlier\"].get(\"regressor_names\"):\n",
    "                \n",
    "                outlier_names = ensure_list(spm_model[\"outlier\"][\"regressor_names\"])\n",
    "                \n",
    "                col_exists = regressor_df.columns.isin(outlier_names)\n",
    "                if col_exists.sum() > 0:\n",
    "                    chosen_outlier_names = regressor_df.columns[col_exists].tolist()\n",
    "                    outlier_indices = outlier_indices + list(np.ravel(np.where((regressor_df[chosen_outlier_names] != 0).any(axis=1))))\n",
    "                else:\n",
    "                    raise Exception(\"No outliers found.\")\n",
    "\n",
    "            outlier_indices = np.unique(np.array(outlier_indices, dtype=int))\n",
    "            \n",
    "            if run != -1:\n",
    "                outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_run-{run}_outliers.txt\")\n",
    "            else:\n",
    "                outlier_file = os.path.join(outlier_path, f\"sub-{sub}_task-{task}_outliers.txt\")\n",
    "                \n",
    "            outlier_files.append(outlier_file)               \n",
    "            np.savetxt(os.path.join(env['data_path'], outlier_file), outlier_indices, fmt=\"%d\")\n",
    "            \n",
    "        event_path = ensure_exist(env['data_path'], spm_model['event_files'].format(**format_args))\n",
    "\n",
    "        if spm_model.get(\"pmod\"):\n",
    "            event_df = pd.read_csv(os.path.join(env['data_path'], event_path), sep='\\t')\n",
    "            \n",
    "            for trial_type, pmods in spm_model['pmod'].items():\n",
    "                trial_df = event_df.query(f'trial_type == \"{trial_type}\"')\n",
    "                \n",
    "                for pmod in ensure_list(pmods):\n",
    "                    if trial_df[pmod].isna().any():\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has missing values for '{trial_type}' events. They will be replaced by mean.\")                        \n",
    "                    if trial_df[pmod].var() == 0:\n",
    "                        issues.append(f\"{event_path}: '{pmod}' has zero variance for '{trial_type}' events, and will cause error. Consider excluding this run.\")\n",
    "        \n",
    "        if (spm_model.get(\"event_options\") is None) and (spm_model.get(\"pmod_options\") is None):\n",
    "            event_files.append(event)\n",
    "        else:\n",
    "            event_files.append(customize_event(env, job_name, spm_model, event_path))\n",
    "                \n",
    "    job['SpecifySPMModel']['functional_runs'] = functional_runs\n",
    "    job['SpecifySPMModel']['event_files'] = event_files\n",
    "    job['SpecifySPMModel']['regressors'] = regressors\n",
    "    job['SpecifySPMModel']['regressor_names'] = chosen_regressor_names\n",
    "    \n",
    "    if len(outlier_files) > 0:\n",
    "        job['SpecifySPMModel']['outlier_files'] = outlier_files\n",
    "    \n",
    "    job['SpecifySPMModel']['time_repetition'] = job[\"Info\"][\"tr\"]\n",
    "    job['Level1Design']['interscan_interval'] = job[\"Info\"][\"tr\"]\n",
    "    \n",
    "    for key in [\"event_options\", \"pmod_options\", \"outlier\"]:\n",
    "        remove_key(key, job[\"SpecifySPMModel\"])\n",
    "\n",
    "    if job['EstimateContrast'].get('basic_contrasts', False):\n",
    "        all_events = pd.concat([pd.read_csv(os.path.join(env['data_path'], x), sep='\\t') for x in job['SpecifySPMModel']['event_files']], \n",
    "                               ignore_index=True)\n",
    "        \n",
    "        contrasts = [] \n",
    "                               \n",
    "        for trial_type in all_events['trial_type'].unique():\n",
    "            contrasts.append([trial_type, \"T\", [trial_type], [1]])\n",
    "        \n",
    "        for cond, pmods in spm_model.get('pmod',{}).items():\n",
    "            for pmod in ensure_list(pmods):\n",
    "                contrasts.append([f'{cond}x{pmod}^1', \"T\", [f'{cond}x{pmod}^1'], [1]])\n",
    "                \n",
    "        contrasts = contrasts + ensure_list(job['EstimateContrast'].get('contrasts',[]))\n",
    "            \n",
    "        job['EstimateContrast']['contrasts'] = contrasts\n",
    "        del job['EstimateContrast']['basic_contrasts']\n",
    "    \n",
    "    job[\"Info\"][\"sub\"] = sub\n",
    "\n",
    "    for key in [\"job_path\"]:\n",
    "        remove_key(key, job[\"Environment\"])\n",
    "\n",
    "    for key in [\"sub_container\", \"exclude\", \"run\", \"tr\"]:\n",
    "        remove_key(key, job[\"Info\"])\n",
    "                \n",
    "    job_output = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    with open(job_output, 'w') as f:\n",
    "        json.dump(job, f)\n",
    "        \n",
    "    if len(issues) == 0:\n",
    "        print(\"job created\")\n",
    "    else:\n",
    "        print(\"issues found - \\n\\t\" + \"\\n\\t\".join(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_path = os.path.join(env['data_path'], env['job_path'], job_name, 'slurm') \n",
    "os.makedirs(slurm_path, exist_ok=True)\n",
    "try:\n",
    "    os.chmod(slurm_path, 0o0777)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "os.makedirs(os.path.join(slurm_path, 'out'), exist_ok=True)\n",
    "os.chmod(os.path.join(slurm_path, 'out'), 0o0777)\n",
    "\n",
    "for sub in subs:\n",
    "    \n",
    "    slurm_header = []\n",
    "    slurm_header.append(\"#!/bin/bash\")\n",
    "    slurm_header.append(f\"#SBATCH --job-name=sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --output=out/sub-{sub}.job\")\n",
    "    slurm_header.append(f\"#SBATCH --error=out/sub-{sub}.err\")\n",
    "    slurm_header.append(\"#SBATCH --time=5-00:00\")\n",
    "    slurm_header.append(\"\")\n",
    "    slurm_header.append(\"srun \")\n",
    "\n",
    "    data_path = env['data_path']\n",
    "    output_path = env['output_path']\n",
    "    working_path = env['working_path']\n",
    "\n",
    "    json_path = os.path.join(job_path, f\"sub-{sub}.json\")\n",
    "    \n",
    "    cmd = []\n",
    "    cmd.append(\"singularity run --cleanenv\")\n",
    "    cmd.append(f\"-B {script_path}:/worker.py\")\n",
    "    cmd.append(f\"-B {data_path}:/data\")\n",
    "    cmd.append(f\"-B {output_path}:/output\")\n",
    "    cmd.append(f\"-B {working_path}:/working\")\n",
    "    cmd.append(f\"-B {json_path}:/job.json\")\n",
    "    cmd.append(f\"{singularity_path} python /worker.py /job.json\")\n",
    "    \n",
    "    slurm_output = os.path.join(output_path, job_name, f\"sub-{sub}\")\n",
    "    slurm_working = os.path.join(working_path, job_name, f\"sub-{sub}\")\n",
    "    \n",
    "    slurm_footer = []\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(\"\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_output}\")\n",
    "    slurm_footer.append(f\"chmod -R 775 {slurm_working}\")\n",
    "    \n",
    "    slurm_cmd = \"\\n\".join(slurm_header) + \" \\\\\\n  \".join(cmd) + \"\\n\".join(slurm_footer) \n",
    "    \n",
    "    with open(os.path.join(slurm_path, f\"sub-{sub}.job\"), 'w') as f:\n",
    "        f.write(slurm_cmd)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test drive one job (sub-28), copy and paste the following lines in terminal:\n",
      "\n",
      "singularity run --cleanenv \\\n",
      "  -B /data00/projects/megameta/scripts/jupyter_megameta/cnlab_pipeline/cnlab/GLM/first_level_HY.py:/worker.py \\\n",
      "  -B /data00/projects/megameta/stanford_ks2:/data \\\n",
      "  -B /data00/projects/megameta/stanford_ks2/derivatives/nipype:/output \\\n",
      "  -B /data00/projects/megameta/stanford_ks2/working/nipype:/working \\\n",
      "  -B /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/jobs/sub-28.json:/job.json \\\n",
      "  /data00/tools/singularity_images/neurodocker.sqsh python /worker.py /job.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"To test drive one job (sub-{sub}), copy and paste the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\" \\\\\\n  \".join(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\n",
      "\n",
      "cd /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-18.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-12.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-13.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-19.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-14.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-17.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-15.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-16.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-1.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-31.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-3.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-30.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-23.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-29.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-25.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-22.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-21.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-24.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-20.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-32.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-2.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-27.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-33.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-34.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-35.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-37.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-5.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-6.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-7.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-8.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-9.job\n",
      "sbatch -D /data00/projects/megameta/stanford_ks2/models/task-fund_model-funded/slurm -c 8 sub-28.job\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternatively, submit the whole batch by copying and pasting the following lines in terminal:\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"cd {slurm_path}\")\n",
    "for sub in subs:\n",
    "    print(f\"sbatch -D {slurm_path} -c 8 sub-{sub}.job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
